# Bioinformatics
## Bioinformatics Essentials:
	- Programming
		- R basics
			variables
			basic operations
			logic operations
			functions
			data reading and writing
			data frame manipulations
			merging
			Workflow and Projects
			Programming Skills
			- Pipes
			- Functions
			- Vectors
			- Iteration (loops)
			- Graphs
			3. Important (frequent) Types of Data
			- Relational data
				https://r4ds.had.co.nz/relational-data.html
				will give you tools for working with multiple interrelated datasets.

			- Strings:
				https://r4ds.had.co.nz/strings.html
				will introduce regular expressions, a powerful tool for manipulating strings.

			- Factors
				https://r4ds.had.co.nz/vectors.html#factors-1
				are how R stores categorical data. They are used when a variable has a fixed set of possible values, or when you want to use a non-alphabetical ordering of a string.

			- Dates and times
				https://r4ds.had.co.nz/dates-and-times.html#dates-and-times
				will give you the key tools for working with dates and date-times.

		- Bash
		- Python

	- Statistics:
		- distributions
		- T test
		- ANOVA
		- correlation
		- regression
		- PCA
		- Multivariate Analysis
		- Bayesian statistics

	- Linear Algebra (very basics, mostly to understand PCA)
		
	- Exploratory Data Analysis (EDA)
		Olhar:
		https://rpubs.com/crazyhottommy/PCA_MDS
		http://girke.bioinformatics.ucr.edu/GEN242/mydoc_Rclustering_3.html
		https://rstudio-pubs-static.s3.amazonaws.com/93706_e3f683a8d77244a5b993b20ad6278f4b.html

		1. Data Wrangling
			- Data Import (https://r4ds.had.co.nz/data-import.html)			
			- Tidy data (https://r4ds.had.co.nz/tidy-data.html)
		2. Data Understand Cycle
		(Transform -> Visualise -> Model -> Transform -> ...)
			- Variation
			- Outliers
			- Covariation
			- Patterns and Models

		3. Communicate
			Data Visualisation
			Low-Dimensional Data Visualisation
				Continuous-Continuous
				Continuous-Categorical
				Categorical-Categorical
				
			High-Dimensional Data Visualisation
				Principal Component Analysis (PCA)
				Multidimensional Scaling (MDS)

	- Bioinformatics Data Retrieval
		- GEO
		- ArrayExpress
		- TGAC

	- Microarray preprocessing:
		- Main technologies: Illumina, Affymetrix and Agilent
		- Quality control
			- ArrayQualityMetrics
			- other technologies

		- Normalization
			- RMA
			- quantile

		- Batch analysis:
			- PCA
			- PVCA
			- RLE
		- Missing values:
			- data imputation
			- variable/sample removal assessment

	- Basic Analysis
		- Differential Gene Expression Analysis
			- limma
	
		- Clustering
			- hierarchical clustering
			- k-means
			- density-based clustering

		- Co-expression Analysis
			- WGCNA
			- CEMITOOL

		- Enrichment Analysis
			- ORA
			- GSEA
			- arbitrary search for Consistent Enrichment Gene AnalySis (asCEGAS)
			- more advanced ones


	- Machine learning:
		- Regression:
			- linear regression
			- polynomial regression
			- ridge/lasso/elastic net regression
			- SVM
			- Decision Trees
			- Random Forest
			- neural networks

		- Classification:
			- logistic regression
			- SVM
			- Decision Tress
			- Random Forest
			- neural networks
		- Model performance evaluation
		- Variable Importance
		- Feature Selection
			- forward/backward selection

	- Dimensionality Reduction
		- Feature Selection/elimination
		- Feature Engineering/extraction

	- Data Integration
		- MixOmics

	- loose things
		- scree plot
		- Oranges
		- Regression Plots
		- gradient descent
		- heteroskedasticity
		- Independent Component Anaysis
		- Factor Analysis
		- Discriminant Analysis



External Resources:
	
	Books:
		- Livro Draghici (microarray analysis bible)



## Differential Expression Analysis

Differential gene expression analysis

<https://www.youtube.com/watch?v=5tGCBW3_0IA>

Normalization

Dispersion estimation

Log fold change estimation

Statistical testing

Filtering

Multiple testing correction

--- NORMALIZATION

for comparing gene expression between (groups of) samples, normalize for

- library size (number of reads obtained)

- RNA composition effect



The number of reads for a gene is also affected by transcript length and GC content

- When studyin differential expression you assume that they stay the same

"FPKM and TC are ineffective and should be definitely abandoned in the context of differential analysis"



"In the presence of high count genes, only DESeq and TMM (edgeR) are able to maintain a reasonable false positive rate without any loss of power



Do NOT use RPKM/FPKM for differential expression analysis!

- Reads (or fragments) per kilobase per million mapped reads.

- Normalizes for gene length and library size:

- 20kb transcript has 400 counts, library size is 20 million reads

=\> RPKM = (400/20)/20 = 1

- 0.5 kb transcript has 10 counts, library size is 20 million reads

=\> RPKM = (10/0.5)/20 = 1



- RPKM/FPKM can be used only for reporting expresison values, not for testing differential expression

- In DE analysis raw counts are needed to assess the measurement precision correctly





--- NORMALIZATION BY edgeR and DESeq

= Aim to make normalized counts for non-differentially expressed genes similar between samples

- do not aim to adjust count distributions between samples

= Assume that

- Most genes are not differentially expressed

- Differentially expressed genes are divided equally between up- anbd down-regulation



= Do not transform data, but use normalization factors within statistical testing



Normalizatio by edgeR/DESeq2 - how?

= DESeq2

- take geometric mean of tene\`s counts across all samples

- divide gene's coutns in a sample by the geometric mean

- take median of these raios -\> sample's normalization factor (applied to read counts)

= edgeR

- Select as reference the sample whose upper quantile is closest to the mean upper quartile

- log ratio of gene\`s counts in sample vs reference -\> M value

- take weighted trimmed mean of M-values (TMM) -\> normalizatio factor (applied to library sizes)

- trim: exlclude genes with high counts or large differneces in expresion

- weights are from the delta method on binomial data <!--#  -->

### Empirical assessment of analysis workflows for differential expression analysis of human samples using RNA-Seq

Unknowns:

\- flow cytometry

	In biotechnology, flow cytometry is a laser or impedance-based, biophysical technology employed in cell counting, cell sorting, biomarker detection and protein engineering, by suspending cells in a stream of fluid and passing them by an electronic detection apparatus.

	- impedance

		Electrical impedance is the measure of the opposition that a circuit presents to a current when a voltage is applied. In quantitative terms, it is the complex ratio of the voltage to the current in an alternating current (AC) circuit.

\- precision & recall

	precision (also called positive predictive value) is the fraction of retrieved instances that are relevant

	proportion of retrieved set that are in fact relevant

	P = Pr(relevant \| retrieved) = TP / (TP + FP)

	intuition: how much junk did we give to the user?

	recall - fraction of all relevant documents that were found

	R = Pr(retrieved \| relevant) = TP / (TP + FN)

	intuition: how much of the good stuff did we miss?

	also known as sensitivity, is the fraction of relevant instances that are retrieved

	Recall in this context is also referred to as the true positive rate or sensitivity, and precision is also referred to as positive predictive value (PPV)

	Accuracy involves how close you come to the correct result and your accuracy improves with tools that are calibrated correctly

	Precision is how consistently you can get that result using the same method

\- Expression Modeler

	swiftly - rapidamente

	paucity - escassez

	thawed - descongelado

	analytes - analitos (Analito é uma substância ou componente químico, em uma amostra, que é alvo de análise em um ensaio)

\- PPLR

1) Background

		- estimating expression from short sequence reads poses unique problems such as accurate read alignment in the presence of sequencing errors, measurement bias depending on library preparation methodology, and complexity in estimating the expression of distinct mRNA transcripts with shared exons.

		- the optimal

	workflow for a given application remains a subject of

	intensive investigation.

		- The three major steps of differential expression analysis by RNA-Seq are:

			+ alignment of reads to an annotated genome (or less commonly, ab initio reconstruction of a transcriptome annotation)

			+ expression modeling to obtain gene-level and/or transcript-level expression estimates

			+ statistical analysis to identify differentially expressed genes or transcripts between comparison groups.

		- the ultimate evaluation of any given tool must take into consideration the samples to which it will be applied and the workflow context in which it will be employed.

		- We find that

	different RNA-Seq analysis workflows differ widely in

	their performance, as assessed by recall, or the propor-

	tion of reference-identified genes that were also identi-

	fied by the given workflow, and precision, or the

	proportion of genes identified by the workflow that were

	also identified by the reference.

		- Many workflows per-

	form equally well, but are calibrated differently with re-

	spect to favoring higher recall or precision, with an

	inverse relationship between these parameters.

		- we recommend that the selection of a

	given approach be guided by the tolerance of down-

	stream applications for type I and type II errors.

2) Methods

	2.1 Samples

	2.2 RNA sequencing

	2.3 Read alignment, expression modeling, and differential expression identification

		all code are available at <https://github.com/cckim47/kimlab/tree/master/rnaseq.>

		Reads were aligned to release GRCh37 of the human genome.

		Reads were aligned with:

			- Bowtie2

			- HISAT2

			- Kallisto

			- Salmon

			- Sailfish

			- SeqMap

			- STAR

			- TopHat2

		Gene and transcript expression was estimated with:

			- BitSeq

			- cufflinks

			- htseq

			- IsoEM

			- Kallisto

			- RSEM

			- rSeq

			- Sailfish

			- Salmon

			- STAR

			- Stringtie

			- eXpress

		Expression matrices for differential expression input were generated using custom scripts as well as the prepDE.py script provided at the Stringtie website.

		Differentially expressed genes or transcripts were identified with Ballgown, baySeq, BitSeq, cuffdiff, DESeq2, EBseq, edgeR exact test, limma coupled with vst or voom transformation, NBPseq, NOISeqBIO, SAMseq and Sleuth.

			Of these, all but Ballgown, BitSeq, NBPSeq, SAMSeq, and Sleuth used intrinsic filtering or recommended extrinsic filtering of genes or transcripts prior to testing.

			For Sailfish and Salmon, outputs were converted to a Sleuth-ready format using wasabi [55].

			For Kallisto, Sailfish, Salmon, and BitSeq, transcript-level values were condensed to gene-level values using tximport prior to evaluating gene-level differential expression [56].

		For all differential expression analyses performed at the transcript-level, significant transcripts were converted to the corresponding gene for performance evaluation, such that if a single transcript was called as differentially expressed, the corresponding gene was also called differentially expressed.

		All software was run at a detection level of alpha of 0.05, FDR of 0.05, or PPLR in the most extreme 0.05.

	2.4 Preparation of reference datasets

		series matrix files were:

			1. downloaded from the NCBI Gene Expression Omnibus

			2. log 2 transformed if necessary

			3. full-quantile normalized

				Smyth GK. Linear models and empirical bayes methods for assessing differential expression in microarray experiments. Stat Appl Genet Mol Biol. 2004;3:1--25.

			4. analyzed for statistically significant gene expression between classical and nonclassical monocytes.

				To reduce bias introduced by a single statistical method, we employed two approaches:

					- Significance Analysis of Microarrays (SAM) [58] with a false discovery rate of 0.05

						Tusher VG, Tibshirani R, Chu G. Significance analysis of microarrays applied to the ionizing radiation response. Proc Natl Acad Sci U S A. 2001;98:5116--21.

					- limma [59, 60],with a BH-adjusted p-value of 0.05.

						Kim CC, Falkow S. Significance analysis of lexical bias in microarray data. BMC Bioinformatics. 2003;4:12.

						Smyth GK. Limma: linear models for microarray data. In: Gentleman R, Carey VJ, Huber W, Irizarry RA, Dudoit S, editors. Bioinforma. Comput. Biol. Solut. Using R bioconductor [internet]. New York, NY: Springer New York; 2005. p.397--420. Available from: <http://dx.doi.org/10.1007/0-387-29362-0_23.>

				Performance of the workflows against both SAM and limma were compared to one another and found to exhibit good reproducibility regardless of the

				statistical method used to generate the data; as such, we chose to use the genes at the intersection of the two methods for our final reference gene sets.

	2.5 Quantification of recall and precision

		Because absolute recall and precision values are influenced by the repertoire of analytes that can be measured by a given platform, they used only the common genes.

		Recall was calculated as the number of significant genes in the intersection of the test RNA-Seq datasetwith the reference dataset, divided by the number of genes

		identified as significant in the reference dataset.

		Precision was calculated as the number of significant genes in the intersection of the test RNA-Seq dataset with the reference dataset, divided by the number of genes identified as significant in the test RNA-Seq dataset.

3) Results and discussion

	3.1 Generation of a real-world RNA-Seq dataset for benchmarking

	3.2 Overview of empirical testing

		we found that performance of various RNA-Seq workflows was remarkably consistent across all four reference datasets.

		We note, however, that these reference datasets are also subject to the inherent biases of the experimental and computational methods used to produce them.

	3.3 Differential influence of workflow stages

		In general, more significant genes were observed when evaluations were performed at the transcript level, because there are more transcripts than genes to potentially be differentially expressed.

		we observed substantial variability in the number of differentially expressed genes identified (n = 208 to 9,489 significant genes)

		Beyond the overall variation, two trends were apparent when the number of genes identified was examined on a by-tool basis:

			1. the differential expression tool had a larger impact on the number of genes identified than the read aligner and expression modeler

				Consequently, the coefficient of variation of the medians was largest for differential expression tools, as compared to read aligners and expression modelers, when assessed at both the gene level (20.5 versus 9.9 and 9.8, respectively) and the transcript level (43.4 versus 10.8 and 39.3).

			2. differential expression tools varied in their robustness to different inputs, with some tools exhibiting relatively reproducible predictions regardless of 	the read aligner and expression modeler choices and expression units (e.g., Ballgown), and other differential expression analysis tools exhibiting a wide 	 range of predictions as the input parameters varied.

		We also evaluated performance of the workflows by calculating recall (intersecting significant genes divided by total number of significant reference genes) and precision (intersecting significant genes divided by total number of significant genes identified by RNA-Seq), using the microarray datasets as references:

			for both precision and recall, the largest effects were observed in workflows differing in the statistical analysis of differential expression, as indicated by the increased medians of differences for this step

	3.4 heterogeneiry in performance characteristics of different workflows

		Recall across the workflows was highly correlated with the number of genes identified (Fig. 5a, b). This was true regardless of which of the reference datasets was used for comparison

		The relative rankings of the workflows, ordered by absolute recall value, tended to be consistent across reference datasets

			For gene-level predictions, a subset of workflows using SAMseq exhibited the highest recall values;

			for transcript-level predictions, workflows using baySeq and NBPSeq exhibited the highest recall.

			However, there were exceptions to these rules, depending on the choice of read aligner and expression modeler.

		Precision was highly inversely correlated with the number of genes predicted across the workflows

		Rankings were generally consistent regardless of which reference dataset was used, as was the overall relationship between significant genes and precision

			For gene-level predictions, a subset of workflows using NOISeqBIO exhibited the highest precision, whereas for transcript-level predictions those with the highest precision used several different combinations of tools, with the most prevalent being Ballgown and NOISeqBIO. Strikingly, when used on transcript-level data, the commonly used combination of TopHat2, cufflinks and cuffdiff exhibited one of the highest precision values, coupled with the second lowest number of differentially expressed genes identified

	3.5 Performance tradeoff

		the workflows employing NOISeqBIO that exhibit the highest precision were also among those with the lowest recall

		An investigation of the relationship between precision and recall revealed that this tradeoff generally persisted throughout, with many workflows following an inverse linear relationship between precision and recall.

			This held true for both gene- and transcript-level analysis, was true regardless of the expression estimation units, and was also consistent across reference datasets

		the differential expression step had the greatest impact on the performance of each workflow along the spectrum of recall and precision.

		Specific tools that tended to track along this linear tradeoff were Ballgown, DESeq2, limma + voom, limma + vst and SAMseq;

		baySeq and EBseq consistently deviated the furthest.

		SAMseq, one tool with a nonparametric approach, has been highlighted as a high performer previously, in particular when there are a large number of replicates available to approximate the underlying distribution, as is the case here; it performs well, though it does exhibit a tendency toward higher recall at the expense of precision.

		NOISeqBIO, the other tested differential expression tool that assumes a nonparametric distribution, has previously been observed to identify fewer differentially expressed genes with larger sample sizes; we also observe this, as well as correspondingly low recall values.

		Of the differential expression methods tested, baySeq and EBseq are the most similar to each other in underlying statistical methodology; both use an underlying negative binomial model, and then estimate a posterior probability of being differentially expressed for each gene. The observation that EBseq deviated furthest from the precision/recall performance line, due to decreased precision without gains in recall, is similar to previous observations showing that EBSeq tended to produce many false positives with large sample sizes. When applied to gene-level data, baySeq performed similarly to EBseq though not as extreme, with relatively low recall without commensurate gains in precision, which may reflect the similarity in their underlying methods.

		All three linear model workflows perform well and track along the linear precision/recall tradeoff, irrespective of upstream processing. However, there is some difference in default tuning, as Ballgown results tended towards higher precision, whereas limma + voom and limma + vst tended towards higher recall.

		Using BitSeq as the expression modeler tended to result in identification of large numbers of differentially expressed genes, but only in combination with

		differential expression tools that used an underlying negative binomial model for expression data (BaySeq, DESeq2, edgeR, and NBPSeq);

		EBSeq was the one exception, with the number of differentially expressed genes within range of workflows using differential expression tools that model other distributions (Ballgown, BitSeq, limma, and NOISeqBIO).

		We note that BitSeq was unusual in that its most prevalent estimated expression count value was between 1 and 2, rather than less than 1 as most expression modelers estimated; this likely explains why these expression data were poorly modeled by a negative binomial distribution.

		using STAR as the read aligner, most notably with Ballgown as the differential expression tool, led to some of the highest performance workflows having a balance of precision and recall. Interestingly, these best performing workflows are not combinations of aligner and estimator that are suggested by the Ballgown authors, demonstrating the utility of broad, empirical exploration for uncovering improved workflows.	

		the selection of a specific workflow should be largely influenced by the tolerance of a specific application for type I versus type II errors. However, it is also important to note that a significant number of workflows deviated from the roughly linear relationship between recall and precision, particularly for tools targeted at gene-level analyses; such workflows could be considered to exhibit lower performance, as higher performance workflows would be available as alternatives at a given recall or precision target value.

		our findings reflect a defined set of parameters, such as read length, sequencing coverage, sample number, and genetic polymorphism. Thus, it is possible that the performance, both absolute and relative, of the above workflows could vary under other conditions, as some studies have observed

			Kanitz A, Gypas F, Gruber AJ, Gruber AR, Martin G, Zavolan M. Comparative assessment of methods for the computational inference of transcript isoform abundance from RNA-seq data. Genome Biol. 2015;16:150.

			Soneson C, Delorenzi M. A comparison of methods for differential expression analysis of RNA-seq data. BMC Bioinformatics. 2013;14:91.

		Importantly, when selecting a pipeline it is essential to consider not only the specific tools selected at each stage of the workflow, but also how they interact with one another.

4) Conclusions

## Network Analysis

### 2008 - GeneMANIA: a real-time multiple association network integration algorithm for predicting gene function

\- A new algorithm that is as accurate as the leading methods while capable of predicting protein function in real-time.

	- use a fast heuristic algorithm, derived from ridge regression, to integrate multiple functional association networks and predict gene function from a single process-specific network using label propagation.

	- gene function prediction through extrapolation of the functional properties of known genes.

	- Genes with similar patterns of expression, synthetic lethality, or chemical sensitivity often have similar functions.

	- function tends to be shared among genes whose gene products interact physically, are part of the same complex, or have similar three-dimensional structures.

	- Computational analyses have also revealed shared function among genes with similar phylogenetic profiles or with shared protein domains.

	- more accurate predictions can be made by combining multiple heterogeneous sources of genomic and proteomic data.

	- guilt-by-association principle.

	

### 2010 - The GeneMANIA prediction server: biological network integration for gene prioritization and predicting gene function

Questions

Points

	- input: gene list

	- GeneMANIA extends the user's list with genes that are functionally similar, or have shared properties with the initial query genes, and displays an interactive functional association network, illustrating the relationships among the genes and data sets.

	- Users interested in prioritizing gnes for planning a functional screen can use GeneMANIA to return ranked lists of genes likely to share phenotypes with those in the query list based on GeneMANIA's large and diverse data collection

	- it assigns weights to data sets based on how useful they are for each query.

		-\> Individual datasets are represented as networks, and in the basic algorithm, each network is assigned a weight primarily based on how well connected genes in the query list are to each other compared with their connectivity to non-query genes.

		-\> GeneMANIA's adaptive wwigting methods also detect and down-weight redundant networks. This is useful for determining how genes in a gene list are connected to one another, or for determining which types of functional genomic data are most useful to collect for finding more genes like those in the query list.

	- Organisms and identifiers:

		-\> support six organism:

			- yeast (Saccharomyces cerevisiae)

			- worm (Caenorhabditis elegans)

			- fly (Drosophila melanogaster)

			- mouse (Mus musculus)

			- Arabidopsis thaliana

			- human (Homo sapiens)

		-\> 747 data sets:

			- 276 co-expression networks, from GEO;

			- 232 physical interaction, from BioGRID;

			- 24 genetic interaction, from BioGRID;

			- 14 co-localization, from Pathway Commons;

			- 5 pathway, from Pathway Commons;

			- 176 predicted protein domain information, from I2D;

			- 12 shared protein domain information), from I2D;

		-\> upport standard genes symbols:

			- Ensembl, Entrez, UniProtKB, and RefSeq database identifiers; and unique gene synonyms.

			- Since we use Ensembl as a primary identifier source, we do not recognize ambiguous gene names that map to multiple Ensembl genes within the same organism.

	- Users can upload their own data sets

	- Network weighting methods

		- By default the GeneMANIA prediction server uses one of two different adaptive network weighting methods:

			- For longer lists, GeneMANIA uses the basic weigting method (GeneMANIA Entry-1 or assigned based on query genes) and weights each network so that after the networks are combined, the query genes interact as myuch as possible with each other while interacting as little as possible with genes not in the list.

			- GeneMANIA learns from longer gene lists, allowing a gene list-specific network weighting to be calculated. Shorter gene lists do not contain enoough information for GeneMANIA to learn which networks mediate the underlying functional relationship among the genes.

			- For shorter lists, GeneMANIA uses a similar principle to weight networks, but tries to reproduce Gene Ontology Biological Process co-annotation patterns raher then the gene list.

			- The user may choose other adptive and non-adaptive weighting methods in the advanced options panel.

			- The two non-adaptive methods are the most conserative options and work well on small gene lists. These methods allow users to choose either to weight each class of network equally.

			- Network weights can also be assiged based on how well they reproduce GO co-annotation patterns for that organism in the molecular function, biological process or cellular component hierachies.

			- The annotation-based weighting may sligtly inflate weights for networks on which current annotations are based or for networks that were derived based on co-annotation patterns of genes. The networks most affected by this inflation are the older, smaller scale protein and genetic interaction studies and networks classified as 'predicted'. However, this inflation does not seem to have a large impact on weights and may be largely avoided by only using networks derived from high-througput assays with the annotation-based schemes.

		- Determining the network weights

			- The constructed composite network is a weighted sum of individual data sources;

			- each edge (link) in the composite network is weighted by the corresponding individual data source.

			- The network weights are non-negative, sum to 100% and reflect the relevance of each data source for predicting membership in the query list.

			- Given the composite network, we use label propagation (12) to score all non-query genes.

			- These scores are used to rank the genes. The score assigned to each gene reflects how often paths that start at a given gene node end up in one of the query nodes and how long and heavily weighted those paths are.

		- Other gene function prediction programs

			- N-Browser

				- functions as a Java web start, which is less convenietn for the casual user

			- bioPIXIE

				- provide users with a fixed network (or networks) to query, built by incorporating multiple yeast and mouse genomic data sets.

			- MouseNet

				- provide users with a fixed network (or networks) to query, built by incorporating multiple yeast and mouse genomic data sets.

			- STRING

				- gives users little choice about which functional association network data to use for their query.

			- Functional Coupling (FunCoup)

				- assigbs weights using a naive Bayes frameowork that cannot detect redundancy among data sets.

## Multidimensional scaling

	- Multidimensional scaling (MDS) is a means of visualizing the level of similarity of individual cases of a dataset.

	- It refers to a set of related ordination techniques used in information visualization, in particular to display the information contained in a distance matrix.

	- It is a form of non-linear dimensionality reduction.

	- An MDS algorithm aims to place each object in N-dimensional space such that the between-object distances are preserved as well as possible.

		- Each object is then assigned coordinates in each of the N dimensions.

		- The number of dimensions of an MDS plot N can exceed 2 and is specified a priori.

		- Choosing N=2 optimizes the object locations for a two-dimensional scatterplot

## Next-Generation Sequencing

Technologies:

	- Illumina (Solexa) sequencing

	- Roche 454 sequencing

	- Ion torrent: Proton/PGM sequencing

	- SOLiD sequencing

	Illumina sequencing

		In Illumina sequencing, 100-150bp reads are used.

	454 sequencing

		Can sequence much longer reads than Illumina. Like Illumina, it does this by sequencing multiple reads at once by reading optical signals as bases are added.

	Ion Torrent: Proton/PGM sequencing

		Unlike Illumina and 454, Ion torrent and Ion proton sequencing do not make use of optical signals. Instead, they exploit the fact that addition of a dNTP to a DNA polymer releases an H+ ion.

		The fragment is \~200bp.

		Like 454, the slide is flooded with a single species of dNTP, along with buffers and polymerase, one NTP at a time. The pH is detected is each of the wells, as each H+ ion released will decrease the pH. The changes in pH allow us to determine if that base, and how many thereof, was added to the sequence read.

### RNA-Seq Analysis

#### STAR

1) Basic Workflow

	1. Generate genome indexes files

		FASTA + GTF -\> genome indexes

	2. Mapping reads to the genome

		genome indexes + RNA-Seq reads (FASTA/FASTQ) -\> :

			- alignments (BAM/SAM)

			- mappring summary statistics

			- splice junctions

			- unmmaped reads

			- signal (wiggle) tracls

			- ...

### microbiome data analysis

REFs:

		- An introduction to the downstream analysis with R and phyloseq

			<https://micca.readthedocs.io/en/latest/phyloseq.html>

		- Microbiota Analysis in R

			<https://rstudio-pubs-static.s3.amazonaws.com/268156_d3ea37937f4f4469839ab6fa2c483842.html>

#### microbiome data analysis - mixOmics

Supervised Analysis and Selection of Discriminative OTUs with sPLS-DA

PLSDA

	1. run the perf function with a PLS-DA model with no variable selection.

		1.1 try to find a good PCs -\> ncomp

	2. assess the performance of the PLSDA on ncomp components

		2.1 decrease in the classification error rate \<-\> increase in classification performance

		2.2 plot indicates a increase/decrease in the classification error rate from one component to ncomp components in the model?

		- BER: Balanced Error Rate

			- should be considered when we have an unbalanced number of samples per group.

			- Are the number of samples per group is similar?

				- if yes, then both overall and BER should be overlapping.

		- Where the performance reaches its best (ncomp)?

			- the of PC w/ the best performance should be used for a final PLSDA model

sPLSDA

	1. Tuning sPLS-DA

		1.1 Parameters to choose in sPLS-DA:

			- the number of variables to select (keepX)

			- the number of components (ncomp)

		1.2 To do this use function tune.splsda()

			- needs to be performed prior to the sPLS-DA analysis to choose the parameters on a grid of keepX values

			

			- make sure to:

				- choose the appropriate M fold cross-validation

				- provide sufficient nrepeat in the evaluation model, except for 'loo' where it can only be run on 1 repeat

				- also check the stability of the features selected during the cross-validation process.

			- may show some convergence issues for some of the cases, it is ok for tuning

	2. Selecting best of PC:

		2.1 look in the graph and see if the addition of components increases/decreases the value BER.

		2.2 choose the best number of components

		3.2 select keepX

	3. run classic sPLS-DA w/ best_ncomp

	4. Evaluating sPLS-DA

		4.1 classification performance of the sPLS-DA multilevel model wit perf()

		4.2 perf()

			- output:

				- mean error rates per component

				- type of distance

			- Here do not hesitate to increase the number of repeats for accurate estimations.

OTU selection and plots

	1. Variable importance

		1.1 selectVar(res, comp = comp_x)\$value

			- outputs:

				- the first selected OTUs

				- their coefficient from the loading vector (value.var)

					- absolute coefficient value: indication of the importance of the OTU in the microbial signature

					- coefficient sign: indicates positive/negative correlations between the OTUs, relatively to the proportions of the others

		1.2 Combine the variable importance from selectVar() with their stability

			- stability: how often were they selected across the different CV runs

	2. Contribution plots - plotLoadings()

		- displays:

			- the abundance of each OTU (large abundance = large absolute value)

			- in which body site they are the most abundant for each sPLS-DA component.

		- They need to be interpreted in combination with the sample plot to:

			- understand the similarities between body sites

			- to answer 'which bacteria characterise those body sites?'

	3. Clustered Image Map - cim()

		- A heatmap will also help understanding the microbial signature.

		- We represent clustered image maps (with Euclidian distance, Ward linkage set by default) for the OTUs selected on each sPLS-DA component.

		- The abundance values that are displayed are the normalised, log ratio transformed values.

		- All OTUs selected by the sPLS-DA model are displayed, other options can include a specific component, or a specific cutoff of 'association', see ?cim.

#### preprocessing

\- Raw data

	- counts [0 - inf]

	- 0 values:

		- Total Sum Scaling normalisation is OK with but not the log ratio transformation

		- apply offset of 1: data.raw = data.raw + 1

		- the offset will not circumvent the zero values issue, as after log ratio transformation we will still have zero values

		- check if the offset was applied:

			sum(which(data.raw == 0))

\- pre-filtering

	- pre-filtering out OTUs with low percentage of reads in relation with the total amount of reads of the data set:

		keep.otu = which(colSums(data)\*100/(sum(colSums(data))) \> percent)

\- check library size

	- ensure that:

		- the number of counts for each sample is relatively similar

		- there is no obvious outlier sample. Those samples may also appear as outliers in PCA plots downstream.

\- Normalisation

	- Because of uneven sequencing depths, library sizes often differ from one sample to another.

	- Two types of scaling / normalisation currently exist to accommodate for library size:

		- TSS (Total Sum Scaling) normalisation which needs to be followed by log ratio transformation

		- CSS (Cumulative Sum Scaling) normalisation followed by log transformation.

	- TSS

		- Is a popular approach to accommodate for varying sampling and sequencing depth.

		

		- In TSS the variable read count is divided by the total number of read counts in each individual sample:

			- each variable read count is divided by the total number of read counts:

				TSS.divide = function(x){ x/sum(x) }

			- function is applied to each row (i.e. each sample):

				data.TSS = t(apply(data.filter, 1, TSS.divide))

		

		- results in compositional data (or proportions) that are restricted to a space where the sum of all OTU proportions for a given sample sums to 1.

		

		- TSS normalisation reflects relative information, and the resulting normalised data reside in a simplex (bounded) rather than an Euclidian space.

		- Using standard statistical methods on such data may lead to

		spurious results and therefore the data must be further transformed.

		

		- to circumvent this issue, we transfor the compositional data using log ratios such as:

			- ILR (Isometric Log Ratio) transformation

			- CLR (Centered Log Ratio) transformation

	- Log ratio transformation

		- ILR and CLR transformations are implemented directly into our multivariate methods pca, plsda and splsda

		- Which log ratio transformation to use?

			- According to Filmozer et al [2]:

				- ILR transformation is best for a PCA analysis.

				- for a PLS-DA and sPLS-DA analysis, logratio = 'CLR' is necessary for OTU selection

				- Generally speaking, a PCA with either TSS+CLR or TSS+ILR may not make much differences visually, but the transformed data will be different.

				- We generally prefer the 'CLR' log ratio transformation as it is faster and can be used consistent throughout our mixMC framework (from PCA to sPLSDA).

		- What if I want to apply another method (multivariate or univariate) on the ILR or CLR data?

			- In that case use our external function "logratio.transfo"

			- Make sure you apply it to the TSS(data.raw +1) first, it will be easier than having to add a small offset by using the loratio.transfo function.

			- The log ratio transformation is crucial when dealing with proportional data!, unless the compositional nature of the data is accounted for directly in the statistical methods.

	- CSS & Log Transformation

		- CSS normalisation was specifically developed for sparse sequencing count data by Paulson et al.

		- CSS can be considered as an extension of the quantile normalisation approach and consists of cumulative sum up to a percentile determined using a data-driven approach.

		- CSS corrects the bias in the assessment of differential abundance introduced by TSS and, according to the authors, would partially account for compositional data.

		- Therefore, for CSS normalised data, no ILR transformation is applied as we consider that this normalisation method does not produce compositional data per say. A simple log transformation is then applied.

## OMICS

### integrOmics an R package to unravel relationships between two omics datasets

integrOmics efficiently performs integrative analyses of two types of 'omics' variables that are measured on the same samples. It includes:

	- a regularized version of canonical correlation analysis to enlighten correlations between two datasets

	- a sparse version of partial least squares (PLS) regression that includes simultaneous variable selection in both datasets

BACKGROUND

	- the simultaneous analysis of two datasets is an important task to better understand the relationships between different biological functional levels.

	- integration of 'omics' data will provide a better understanding of biological systems

	- challenges:

		- computational issues because of the "large p, small n" problem, e.g. canonical correlation analysis (CCA; Hotelling, 1936)

		- give interpretable results, i.e. to ansewer the following questions:

			(i) which variables from both types are related to each other

			(ii) which relevant variables provide more insight into the biological experimental hypotheses?

		-\> The solution is to perform variable selection while combining the two types of variables in the modeled integration process.

	to adress this problem, they developed two approaches:

		- a regularized version of CCA to overcome computational issues in CCA when p\>\>n (González et al., 2009)

		- a variant of partial least squares (PLS) regression (Wold, 1966) called sparse PLS (Lê Cao et al., 2008, 2009) to simultaneously integrate and select variables using lasso penalization (Tibshirani, 1996)

METHODS AND IMPLEMENTATION

	CCA and PLS are both exploratory approaches which enable the integration of two datasets, but they fundamentally differ in essence:

		- CCA maximizes the correlation between linear combinations of the variables from each dataset

		- PLS maximizes the covariance

	Vinod (1976) and González et al. (2008) introduced l2 penalties on the covariance matrices so as to make them invertible in a ridge CCA (rCCA).

	PLS circumvents this ill-conditioned matrices issue by performing local regressions

	Both approaches seek for:

		(i) p- and q-dimenstional weight vectors, called canonical factors or loading vectors

		(ii) n-dimensional vectors, called score or latent vectors

	In order to give interpretable results and remove noisy variables, Lê Cao et al. (2008, 2009) proposed to add l1 penalizations to each PLS loading vector, in which the magnitude of the coeffiecients indicate the importance of the variables in the integrative model.

		-\> as a result, many coefficients in these vectors are set to zero, which naturally allows for a simultaneous variable selection in the two datasets.

	Two types of analysis were proposed in sPLS:

		- regression analysis fro a causal relationship between the two datasets

		- canonical analysis for a reciprocal relationship similar to a CCA framework

SOFTWARE FEATURES

	- The Q² criterion (Tenenhaus, 1998) can be computed to determine the number of components to choose from the (s)PLS regression model

	- The root mean square error prediction can be used to choose the optimal number of variables to be selected using cross-validation

	- the user can also estimatee the predicted value of a new sample in the model

	- regularization parameters in rCCA can be tuned using cross-validation.

	- missing values of each dataset can be efficiently imputed with a singular value decomposition using NIPALS, an interative version of principal component analysis (Wold, 1966)

	Visualization outputs:

		- scatter plots of the score (latent) vectors from the first dimensions allow the user to identify similarities between the samples. Often, these similarities (clusters of samples) were found to have biological meaning (González et al., 2009;; Lê Cao et al, 2009)

		- further, the (selected) variables can be represented by projecting them on correlation circles to hiighlight their correlation strucutre (González et al., 2008)

		- enables the inference of large-scale association networks between the two datasets with the use of network graphical displays, where the edges represent relevant associations between the variables (nodes)

	Versatility of integrOmics:

		- rCCA and sPLS have been successfully applied in various biological contexts where p+q\>\>n

		- sPLS: integrate gene expression with metabolite expression, clinical chemistry or fatty acids measurements

		- canonical rCCA/sPLS: relate physicochemical measurements with sensory variables or to relate gene expression measured on different platforms

### Visualising associations between paired "omics" data sets

Major challenge with the integration of omics data -\> extraction of discernable biological meaning from multiple omics data

	

	Multivariate approaches:

		- Aim: unravell the correlation structure between two sets of data measured on the same samples

		- achieve dimension reduction by summarizing the data into a small number of components or variates, which are linear combinations of the original variables

		- exploiting coexpression between disparate types of biological measures instead of differential expression.

	Other methods:

		- clustering tehcniques

			- simple criteria matching: order the variables according to fold-change or univariate statistical tests for a given threshold. These variables are considered to be "clustered"

			- self-organizing maps, use Euclidian distances. However, they are known to encounter difficulties in finding variables "negatively" (oppositely) associated with each other [10]

			- comprehensively compare all variables against each other using a similarity measure, such as Pearson correlation coefficient [10,11], or mutual information [9]. Once these associations are graphically represented, the aim is to obtain fresh insights into the different biological functional levels, which then act as a foundation for new hypotheses.

	Correlation Circle plots

		- Correlation Circle plots were primarily used for PCA outputs to visualise the relationship between variates and variables of the same type, where one single omics data set is analysed [19-23].

		- enables a graphical examination of the relationships between variables and variates.

		- the coordinates of the variables are obtained by calculating the correlation between each original variable and their associated component

		- Because variables are usually centered and standardized, the correlation between each variable and a component is simply the projection of the variable on the axis defined by the component.

		- variables can be represented as vectors (see Figure 1(b)) and the relationship (correlation) between the two types of variables can be approximated by the inner product between the associated vectors.

		The inner product is defined as the product of the two vectors lengths and their cosine angle. Thus, the nature of the correlation between two variables can be visualised through the angles between two vectors:

			- if the angle is sharp, the correlation is positive

			- if the angle is obtuse the correlation is negative

			- if the angle is right the correlation is null.

		- The centered and standardized variables are projected onto the space spanned by the two chosen components, inside a circle of radius 1. Thus, from the inner product definition, the longer the distance to the origin, the stronger the relationship between the variables. The variables closely located to the circumference of radius 1 can be directly interpreted, since the closeness on the plane corresponds to the closeness in the d-dimensional variables space.

		- For variables closely located to the origin, it means that some information can be carried on other axes and, it might be necessary to visualise the Correlation Circles plots in the subsequent dimensions

	Relevance Networks

		- A conceptually simple approach for modelling net-like correlation structures between two data sets

		- was introduced by [10] as a tool to study associations between pair of variables coming from several types of genomic data.

		- This method generates a graph where nodes represent variables, and edges represent variable associations.

		- Since the relevance networks are visual representations of the correlations between variables, one looks for clusters or sub-networks of subsets of variables, where the edge colors indicate the nature of the correlation (positive, negative, strong or weak). Each of these clusters often highlight a specific correlation structure between the features.

		- The Relevance Network is built in a simple manner:

			1. the correlation matrix is inferred from the data

			2. for every estimated correlation coefficients exceeding (in absolute value) a prespecified threshold between two variables (say 0.6), an edge is drawn between these two variables, otherwise, no edge is drawn and these two variables are considered not associated for this threshold, and the variables/nodes with no link are not represented in the graph.

		- We will thus display Relevance Networks through the use of bipartite graph (or bigraph), where variables/nodes from X can only be connected to variables/nodes from Y.

		- Instead of computing the Pearson correlation coefficients between each pair of variables as was proposed by [10], bipartite networks are inferred using a pair-wise similarity matrix directly obtained from the outputs of the integrative approaches (regularized) CCA and (sparse) PLS. The values in the similarity matrix are computed as the correlation between the two types of projected variables onto the space spanned by the first components retained in the analysis. The values in the similarity matrix can be seen as a robust approximation of the Pearson correlation

		Advantages:

			- ability to simultaneously represent positive and negative correlations, which are missed by methods using Euclidian distances or mutual information.

			- ability to represent genes in several pathways, and, most importantly for our purpose, to represent correlations across disparate biological measures.

		- Limitations:

			- it requires extensive computing ressources as mentioned by [26] to compute the comprehensive pairwise associations when the underlying network is fully connected

	Clustered Image Maps

		The similarity matrix represented by the CIM is the same as in the relevance networks described above. CIM is a visualisation tool that complements well the Correlation Circles plots and the Relevance Networks as clusters of subsets of variables of the same type correlated with subsets of variables of the other type can be observed.

## Principal Component Analysis

Is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components.

If there are n observations with p variables, then the number of distinct principal components is min(n-1,p)

\- This transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components. The resulting vectors (each being a linear combination of the variables and containing n observations) are an uncorrelated orthogonal basis set.

\- PCA can be done by eigenvalue decomposition of a data covariance (or correlation) matrix or singular value decomposition of a data matrix, usually after a normalization step of the initial data. The normalization of each attribute consists of mean centering -- subtracting each data value from its variable's measured mean so that its empirical mean (average) is zero -- and, possibly, normalizing each variable's variance to make it equal to 1;

component/factor scores: the transformed variable values corresponding to a particular data point

loadings: the weight by which each standardized original variable should be multiplied to get the component score

\- PCA is sensitive to the relative scaling of the original variables.

\- is a dimension-reduction tool that can be used to reduce a large set of variables to a small set that still contains most of the information in the large set.

\- is used to explain the variance-covariance structure of a set of variables through linear combinations.

\- As an added benefit, each of the "new" variables after PCA are all independent of one another. This is a benefit because the assumptions of a linear model require our independent variables to be independent of one another.

\- When to use PCA:

	- do you want to reduce the number of variables, but aren't able to identify variables to completely remove from consideration?

	- do you want to ensure your variables are independent of one another

	- are you comfortable making your independent variable less interpretable?

	If the answer is 'yes' to all three questions, the PCA is a good method to use.

you should have tabular data organized with n rows and p+1 columns, where there is one column corresponding to your dependent variable (usually denoted Y) and p columns corresponding to each of your independent variables (the matrix of which is usually denoted X).

1) Separate your data into Y and X, as defined above --- we'll mostly be working with X.

2) Take the matrix of independent variables X and, for each column, subtract the mean of that column from each entry. (This ensures that each column has a mean of zero.)

3) Decide whether or not to standardize. Given the columns of X, are features with higher variance more important than features with lower variance, or is the importance of features independent of the variance? (In this case, importance means how well that feature predicts Y.) If the importance of features is independent of the variance of the features, then divide each observation in a column by that column's standard deviation. (This, combined with step 2, standardizes each column of X to make sure each column has mean zero and standard deviation 1.) Call the centered (and possibly standardized) matrix Z.

4) Take the matrix Z, transpose it, and multiply the transposed matrix by Z. (Writing this out mathematically, we would write this as ZᵀZ.) The resulting matrix is the covariance matrix of Z, up to a constant.

5) (This is probably the toughest step to follow --- stick with me here.) Calculate the eigenvectors and their corresponding eigenvalues of ZᵀZ. This is quite easily done in most computing packages--- in fact, the eigendecomposition of ZᵀZ is where we decompose ZᵀZ into PDP⁻¹, where P is the matrix of eigenvectors and D is the diagonal matrix with eigenvalues on the diagonal and values of zero everywhere else. The eigenvalues on the diagonal of D will be associated with the corresponding column in P --- that is, the first element of D is λ₁ and the corresponding eigenvector is the first column of P. This holds for all elements in D and their corresponding eigenvectors in P. We will always be able to calculate PDP⁻¹ in this fashion. (Bonus: for those interested, we can always calculate PDP⁻¹ in this fashion because ZᵀZ is a symmetric, positive semidefinite matrix.)

6) Take the eigenvalues λ₁, λ₂, ..., λp and sort them from largest to smallest. In doing so, sort the eigenvectors in P accordingly. (For example, if λ₂ is the largest eigenvalue, then take the second column of P and place it in the first column position.) Depending on the computing package, this may be done automatically. Call this sorted matrix of eigenvectors P\*. (The columns of P\* should be the same as the columns of P, but perhaps in a different order.) Note that these eigenvectors are independent of one another.

7) Calculate Z\* = ZP\*. This new matrix, Z\*, is a centered/standardized version of X but now each observation is a combination of the original variables, where the weights are determined by the eigenvector. As a bonus, because our eigenvectors in P\* are independent of one another, each column of Z\* is also independent of one another!

8) Finally, we need to determine how many features to keep versus how many to drop. There are three common methods to determine this, discussed below and followed by an explicit example:

	- Method 1: We arbitrarily select how many dimensions we want to keep. Perhaps I want to visually represent things in two dimensions, so I may only keep two features. This is use-case dependent and there isn't a hard-and-fast rule for how many features I should pick.

	- Method 2: Calculate the proportion of variance explained (briefly explained below) for each feature, pick a threshold, and add features until you hit that threshold. (For example, if you want to explain 80% of the total variability possibly explained by your model, add features with the largest explained proportion of variance until your proportion of variance explained hits or exceeds 80%.)

	- Method 3: This is closely related to Method 2. Calculate the proportion of variance explained for each feature, sort features by proportion of variance explained and plot the cumulative proportion of variance explained as you keep more features. (This plot is called a scree plot, shown below.) One can pick how many features to include by identifying the point where adding a new feature has a significant drop in variance explained relative to the previous feature, and choosing features up until that point. (I call this the "find the elbow" method, as looking at the "bend" or "elbow" in the scree plot determines where the biggest drop in proportion of variance explained occurs.)

	Because each eigenvalue is roughly the importance of its corresponding eigenvector, the proportion of variance explained is the sum of the eigenvalues of the features you kept divided by the sum of the eigenvalues of all features.

Why dows PCA work?

	While PCA is a very technical method relying on in-depth linear algebra algorithms, it's a relatively intuitive method when you think about it.

	- First, the covariance matrix ZᵀZ is a matrix that contains estimates of how every variable in Z relates to every other variable in Z. Understanding how one variable is associated with another is quite powerful.

	- Second, eigenvalues and eigenvectors are important. Eigenvectors represent directions. Think of plotting your data on a multidimensional scatterplot. Then one can think of an individual eigenvector as a particular "direction" in your scatterplot of data. Eigenvalues represent magnitude, or importance. Bigger eigenvalues correlate with more important directions.

	- Finally, we make an assumption that more variability in a particular direction correlates with explaining the behavior of the dependent variable. Lots of variability usually indicates signal, whereas little variability usually indicates noise. Thus, the more variability there is in a particular direction is, theoretically, indicative of something important we want to detect.

	Thus, PCA is a method that brings together:

		- A measure of how each variable is associated with one another. (Covariance matrix.)

		- The directions in which our data are dispersed. (Eigenvectors.)

		- The relative importance of these different directions. (Eigenvalues.)

	

	PCA combines our predictors and allows us to drop the eigenvectors that are relatively unimportant.

Are there extensions to PCA?

	- principal component regression:

		where we take our untransformed Y and regress it on the subset of Z\* that we didn't drop. (This is where the independence of the columns of Z\* comes in; by regressing Y on Z\*, we know that the required independence of independent variables will necessarily be satisfied. However, we will need to still check our other assumptions.)

	- kernel PCA

		- Kernel PCA has been demonstrated to be useful for novelty detection and image de-noising

		- use kernel methods:

			- are a class of algorithms for pattern analysis, e.g. SVM

			- the kernel trick means transforming data into another dimension that has a clear dividing margin between classes of data.

			- to solve these task, kernel methods require only a user-specidied kernel, i.e., a similarity function over pairs of data points in raw representation

PCA Assumptions:

	- PCA itself is a nonparametric method, but regression or hypothesis testing after using PCA might require parametric assumptions.

	- We do not have to make any distributional assumptions in order to extract the Principal Component directions from a covariance or correlation matrix.

	- All we require is that the maximizers are unit vectors and perpendicular to the previous directions.

	- We do not need normality for the extraction but we definitely need the normality for hypothesis testing, e.g. to see how many directions are significant. It's worth noting, however, that with normality we have a pretty neat interpretation of PCA as the axes in ellipsoids of constant density (recall the exponent of a multivariate Normal distribution).

resources:

	- <http://setosa.io/ev/principal-component-analysis/>

	- <https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab>

## MixOmics

### DIABLO

N-Integration discriminant analysis with DIABLO

1) Data Input

The data for mixDIABLO need to be set up as a list of data matrices matching the same samples.

The blocks:

	are 'omics data sets, where each row matches to the same biological sample from one data set to another.

	The outcome Y is set as a factor for the supervised analysis. Here Y corresponds to the PAM50 classification.

2) Tunning

	- Matrix design

		determines which blocks should be connected to maximize the correlation or covariance between components.

		the values may range between 0 (no correlation) to 1 (correlation to maximize) and is a symmetrical matrix.

		The design can be chosen based on:

			- prior knowledge ('I expect mRNA and miRNA to be highly correlated')

			- data-driven (e.g. based on a prior analysis, such as a non sparse analysis with block.pls to examine the correlation between the different blocks via the modelled components).

		Our experience has shown that a compromise between maximising the correlation between blocks, and discriminating the outcome needed to be achieved, and that the weight in the design matrix could be set to \< 1 between blocks

	- Number of components

		fit a DIABLO model without variable selection to assess the global performance and choose the number of components for the final DIABLO model.

	- keepX

		- should be used to tune the keepX parameters in the block.splsda function.

		- We choose the optimal number of variables to select in each data set using the tune function, for a grid of keepX values

		- the function has been set to favor the small-ish signature while allowing to obtain a sufficient number of variables for downstream validation / interpretation. See ?tune.block.splsda.

		- 	We choose the optimal number of variables to select in each data set using the tune.block.splsda function, to which we provide a grid of keepX values for each type of omics

		- the function has been set to favour the small-ish signature while allowing to obtain a sufficient number of variables for downstream validation / interpretation.

		- The number of features to select on each component is returned in tune.TCGA\$choice.keepX.

			Alternatively, you can manually input those parameters as indicated below.

3) Performance

	- We assess the performance of the model using perf() function.

	- The method runs:

		1. block.splsda model on the pre-specified arguments input from our sgccda.res object but on cross-validated samples

		2. assess the accuracy of the prediction on the left out samples.

	- Outputs:

		- usual (balanced) classification error rates

		- predicted dummy variables and variates

		- stability of the selected features

		- performance based on:

			- Majority Vote (each data set votes for a class for a particular test sample)

			- weighted vote, where the weight is defined according to the correlation between the latent component associated to a particular data set and the outcome.

	- Since the tune function was used with the centroid.dist argument, we examine the outputs of the perf function for that same distance.

		\> set.seed(123)for reproducibility, only when the \`cpus' argument is not used

		\> perf.diablo = perf(sgccda.res, validation = 'Mfold', M = 10, nrepeat = 10, dist = 'centroids.dist')

		\> perf.diablo lists the different outputs

		\> perf.diablo\$MajorityVote.error.rate Performance with Majority vote

		\> perf.diablo\$WeightedVote.error.rate Performance with Weighted prediction

		1. Prediction on an external test set

			The predict function predicts the class of samples from a test set.

			- In our specific case, one data set is missing in the test set but the method can still be applied.

			- Make sure the name of the blocks correspond exactly.

			prepare test set data: here one block (proteins) is missing

			data.test.TCGA = list(mRNA = breast.TCGA\$data.test\$mrna, miRNA = breast.TCGA\$data.test\$mirna)

			predict.diablo = predict(sgccda.res, newdata = data.test.TCGA)

			the warning message will inform us that one block is missing

			predict.diablo list the different outputs

			- Confusion table

				- The confusion table compares the real subtypes with the predicted subtypes for a 2 component model, for the distance of interest:

					confusion.mat = get.confusion_matrix(truth = breast.TCGA\$data.test\$subtype, predicted = predict.diablo\$WeightedVote\$centroids.dist[,2])

					confusion.mat

					get.BER(confusion.mat)

Plots

	- Samples

		- Arrow Plot - plotIndiv()

			- arrow plot below

			- projects each sample into the space spanned by the components of each block. The optional argument blocks can output a specific data set. Ellipse plots are also available (argument ellipse = TRUE)

			- the start of the arrow indicates the centroid between all data sets for a given sample

			- the tips of the arrows the location of that sample in each block

			- Such graphic highlight the agreement between all data sets at the sample level, when modelled with DIABLO.

	- Variables

		- Circle Plot - plotVar()

			- The correlation circle plot highlights the contribution of each selected variable to each component, see [3].

			- plotVar displays the variables from all blocks, selected on component 1 and 2 (see here for more examples).

			- Clusters of points indicate a strong correlation between variables

		- Circus Plot - circosPlot()

			- The circos plot represents the correlations between variables of different types, represented on the side quadrants.

			- Several display options are possible, to show within and between connexions between blocks, expression levels of each variable according to each class (argument line = TRUE).

			- The circos plot is built based on a similarity matrix, extended to the case of multiple data sets from [3].

		- Relevance Network Plot - network()

			- Another visualisation of the correlation between the different types of variables

			- is also built on the similarity matrix

			- Each color represents a type of variable. A threshold can also be set using the argument cutoff.

			- sometimes the output may not show with Rstudio because of margin issues. The plot can be saved as an image using the argument save and name.save. An interactive argument is also available for the cutoff argument, see details in ?network.

			- The network can be saved in a .gml format to be input into the software Cytoscape, using the R package igraph:

				\> library(igraph)

				\> my.network = network(sgccda.res, blocks = c(1,2,3), color.node = c('darkorchid', 'brown1', 'lightgreen'), cutoff = 0.4)

				\> write.graph(my.network\$gR, file = "myNetwork.gml", format = "gml")

		- Variable Importance Plot- plotLoadings()

			- visualizes the loading weights of each selected variables on each component and each data set.

			- The color indicates the class in which the variable has the maximum level of expression (contrib = 'max'), on average (method = 'mean') or using the median (method = 'median').

	- Samples and Variables

		- Clustered Image Map Plot - cimDIABLO()

			- is a clustered image map specifically implemented to represent the multi-omics molecular signature expression for each sample.

	- Diagnostic

		- DIABLO Diagnostic Plot - plotDIABLO()

			- is a diagnostic plot to check whether the correlation between components from each data set has been maximized as specified in the design matrix

			- The colors and ellipses related to the sample subtypes and indicate the discriminative power of each component to separate the different tumour subtypes.

			- plotDiablo(sgccda.res, ncomp = 1)

			- Are the first components from each data set highly correlated to each other?

	- Performance

		- AUC Plot - auroc()

			- An AUC plot per block

			- Refer to [5] for the interpretation of such output as the ROC and AUC criteria are not particularly insightful in relation to the performance evaluation of our methods, but can complement the statistical analysis.

### EXPLORATORY DATA ANALYSIS WITH mixOMICS

SINGLE-OMICS

	PCA

	IPCA

DOUBLE-OMICS

	rCCA

	sPLS

MULTI-OMICS

	DIABLO

	MINT

	

SINGLE-OMICS

	PRINCIPAL COMPONENT ANALYSIS [PCA]

		- Jolliffe, 2005

		- is primarily used to explore one single type of 'omics' data

		- identify the largest sources of variation

		- a mathematical procedure that uses orthogonal linear transformation of data from possibly correlated variables into uncorrelated principal components.

		- the first component explains as much of the variability in the data as possible, and each following PC explains as much of the remaining variability as possible.

		- only the PCs which explain the most variance are retained.

			-\> This is why choosing the number of dimensions or components (ncomp) is crucial (see tune.pca)

		- in mixOmics, PCA is numerically solved in two ways:

			1. with singular value decomposition (SVD) of the data matrix:

				- is the most computationally efficient way

				- also adopted by most softwares and prcomp

			2. with the Non-linear Iterative Partial Least Squares (NIPALS) in the case of missing values, which uses an iterative power method.

		- input data should be centered and possibly (sometimes preferably) scaled so that all variables have a unit variance.

			- this is specially advised in the case where the variance is not homogeneous across variables.

			- by default, the variables are centered and scaled in the function.

		- choosing the optimal parameters

			- we can obtain as many dimensions as the minimun between the samples and variables.

			- however, the goal is to reduce the complexity of the data:

				- summarize the data in fewer underlying dimension

			- the of PC to retain is therefore crucial when performing PCA.

				- The function tune.pca will plot the barplot of the proportion of explained variance for min(n, p)principal components, where n is the number of samples, and p the number of variables.

		sparse Principal Component Analysis (sPCA)

			- Shen and Huang, 2008

			- is an unsupervised and exploratory technique

			- is based on singular value decomposition and is appropriate to deal with large data sets.

			- in mixOmics, 'sparsity' is achieved via LASSO penalizations.

			- sPCA is useful to remove some of the non informative variables in PCA and can be used to investigate whether 'tighter' sample clusters can be obtained and which are the variables that higlhy contribute to each PC

			- the of variables to select on each PC must be input by the user (keepX)

			- tunning sPCA keepX based on the amount of explained variance is difficult (the less variables, including noisy variables, the less variance is explained).

	INDEPENDENT PRINCIPAL COMPONENT ANALYSIS [IPCA]

		Deal w/ some PCA Limitations:

			- PCA assumes that gene expression follows a multivariate normal distribution

				-\> recent studies have demonstrated that microarray gene expression follow instead a super-Gaussian distribution

			- PCA decomposes the data based on the maximization of its variance

				-\> in some cases, the biological question may not be related to the highest variance in the data.

		- combines the advantages of both PCA and Independent Component Analysis (ICA)

		- It uses ICA as a denoising process of the loading vectors produced by PCA to better highlight the important biological entities and reveal insightful patterns in the data.

		- The algorithm is as follows:

			1. the original data matrix is centered (by default)

			2. PCA is used to reduce dimension and generate the loading vectors

			3. ICA (FastICA) is implemented on the loading vectors to generate independent loading vectors

			4. The centered data matrix is projected on the independent loading vectors to obtain the independent principal components.

		- offers a better visualization of the data than ICA and with a smaller number of components than PCA

		- Choosing the optimal parameters

			- the of variables to select is still an open issue

			- Yao et al (2012) proposed to use the Davies Bouldin measure

				- is an index of crisp cluster validity

				- this index compares the within-cluster scatter with the between-cluster separation

			- Kurtosis

				- the kurtosis measure is used to order the loading vectors to order the Independet Principal Components.

				- is a good post hoc indicator of the number of components to choose, as a sudden drop in the values corresponds to irrelevant dimensions.

PLS-DA

Multilevel

Missing Values

### - Independent Component Analysis (ICA)

	- Resouces:

		- <http://arnauddelorme.com/ica_for_dummies/>

		

	- independent component analysis (ICA) is a computational method for separating a multivariate signal into additive subcomponents. This is done by assuming that the subcomponents are non-Gaussian signals and that they are statistically independent from each other. ICA is a special case of blind source separation. A common example application is the "cocktail party problem" of listening in on one person's speech in a noisy room.

	- is a statistical and computational technique for revealing hidden factors that underlie sets of random variables, measurements, or signals.

	- ICA defines a generative model for the observed multivariate data, which is typically given as a large database of samples. In the model, the data variables are assumed to be linear or nonlinear mixtures of some unknown latent variables, and the mixing system is also unknown. The latent variables are assumed nongaussian and mutually independent, and they are called the independent components of the observed data. These independent components, also called sources or factors, can be found by ICA.

	- ICA can be seen as an extension to principal component analysis and factor analysis. ICA is a much more powerful technique, however, capable of finding the underlying factors or sources when these classic methods fail completely.

	- ICA is a technique to separate linearly mixed sources.

	- is quite robust to different degrees of noise

	- in theory, ICA can only extract sources that are combined linearly

	- Steps

		- Whitening the data

			- a first step in many ICA algorithms it so whiten (or sphere) the data: this means that we remove any correlations in the data.

				- Why do that:

					- A geometrical interpretation is that it restores the initial "shape" of the data and that then ICA must only rotate the resulting matrix

			- after whitening, the variance on both axis is now equal and the correlation of the projection of the data on both axis is 0 (meaning that the covariance matrix is diagonal and that all the diagonal elements are equal). Then applying ICA only mean to "rotate" this representation back to the original A and B axis space.

			- The whitening process is simply a linear change of coordinate of the mixed data. Once the ICA solution is found in this "whitened" coordinate frame, we can easily reproject the ICA solution back into the original coordinate frame.

		- The ICA algorithm

			- Intuitively you can imagine that ICA rotates the whitened matrix back to the original (A,B) space. It performs the rotation by minimizing the Gaussianity of the data projected on both axes (fixed point ICA)

			- By rotating the axis and minimizing Gaussianity of the projection, ICA is able to recover the original sources which are statistically independent (this property comes from the central limit theorem which states that any linear mixture of 2 independent random variables is more Gaussian than the original variables).

			- ICA can deal with an arbitrary high number of dimensions:

				- Let's consider 128 EEG electrodes for instance. The signal recorded in all electrode at each time point then constitutes a data point in a 128 dimension space. After whitening the data, ICA will "rotate the 128 axis" in order to minimize the Gaussianity of the projection on all axis (note that unlike PCA the axis do not have to remain orthogonal).

				- What we call ICA components is the matrix that allows projecting the data in the initial space to one of the axis found by ICA. The weight matrix is the full transformation from the original space. When we write S = W X, X is the data in the original space, S is the source activity and W is the weight matrix to go from the S space to the X space. Now the rows of W are the vector with which we can compute the activity of one independent component.

				- when we talk about independent components, we usually refer to two concepts:

					- Rows of the S matrix which are the time course of the component activity

					- Columns of the W-1 matrix which are the scalp projection of the components

	- ICA properties:

		- ICA can only separate linearly mixed sources.

		- Since ICA is dealing with clouds of point, changing the order in which the points are plotted (the time points order in EEG) has virtually no effect on the outcome of the algorithm.

		- Changing the channel order (for instance swapping electrode locations in EEG) has also no effect on the outcome of the algorithm. For EEG, the algorithm has no a priori about the electrode location and the fact that ICA components can most of the time be resolved to a single equivalent dipole is a proof that ICA is able to isolate compact domains of cortical synchrony.

		- Since ICA separates sources by maximizing their non-Gaussianity, perfect Gaussian sources can not be separated

		- Even when the sources are not independent, ICA finds a space where they are maximally independents.

### Independent Principal Component Analysis (IPCA)

	

	Limitations when using PCA:

		- PCA assumes that gene expression follows a multivariate normal distribution and recent studies have demonstrated that microarray gene expression measurements follow instead a super-Gaussian distribution

		- PCA decomposes the data based on the maximization of its variance. In some cases, the biological question may not be related to the highest variance in the data

	

	- IPCA combines the advantages PCA and Independent Component Analysis (ICA):

		- see Yao et al., 2012 [1]

		- It uses ICA as a denoising process of the loading vectors produced by PCA to better highlight the important biological entities and reveal insightful patterns in the data.

	- IPCA offers a better visualization of the data than ICA and with a smaller number of components than PCA.

	- IPCA results in better clustering of biological samples on graphical representations.

	- both IPCA and PCA rank the variables in similar same order of importance, the largest difference lies in the loading values

	- choosin the number of components:

		The kurtosis measure is used to order the loading vectors to order the Independent Principal Components. We have shown that the kurtosis value is a good post hoc indicator of the number of components to choose, as a sudden drop in the values corresponds to irrelevant dimensions.

	sIPCA

		- Sparse Independent Principal Component Analysis (IPCA) combines the advantages of IPCA with soft-thresholding applied in the independent loading vectors to perform internal variable selection

		- The use of a sparse IPCA would be more appropriate to interpret the results as there are too many genes

		- Choosing the number of variables to select:

			- is still an open issue.

			- In our paper we proposed to use the Davies Bouldinmeasure which is an index of crisp cluster validity.

				- This index compares the within-cluster scatter with the between-cluster separation.

	REF:

		- Yao F., Coquery J., Lê Cao K.-A. (2012) Independent Principal Component Analysis for biologically meaningful dimension reduction of large biological data sets, BMC Bioinformatics 13:24.

		- Comon P: Independent component analysis, a new concept? Signal Process 1994, 36:287-314.

		- Hyvärinen A, Oja E: Indepedent Component Analysis: Algorithms and Applications. Neural Networks 2000, 13(4-5):411-430

### mixOmics - An R package for omics feature selection and multiple data integration_2017

univariate statistical analysis:

	- ANOVA

	- linear models

	- t-tests

	Cons:

		- ignores relationships between the different features

		- may miss crucial biological information

		(biological features act in concert to modulate and influence biological systems and signalling pathways)

multivariate statistical analysis:

	- model features as a set

	- can provide a more insightful picture of a biological system

	- can complement the results obtained dfrom univariate methods

mixOmics:

	- data exploration, dimension reduction and visualization of integrated omics data sets

	- multivariate projection-based methodologies

	- supervised analysis

	- DIABLO

		- enables the integration of the same biological N samples measured on different 'omics platforms (N-integration, [11])

		- other types of N-integration:

			- often performed by concatenating all the different 'omics data sets [13], which ignores the heterogeneity between 'omics platforms and mainly highlights one single type of 'omics.

			- combine the molecular signatures identified from separate analyses of each 'omics [14], which disregards the relationships between the different 'omics functional levels.

	- MINT

		- enables the integration of several independent data sets or studies measured on the same P predictors (P-integration, [12]).

		- With P-integration, statistical methods are often sequentially combined to accommodate or correct for technical differences ('batch effects') among studies before classifying samples with a suitable classification method. Such sequential approaches are time consuming and are prone to overfitting when predicting the class of new samples [12].

Data input

	- normalized continuous data

	- we recommend pre-filtering the data to less than 10K predictors per data set, for example by using Median Absolute Deviation [15] for RNA-seq data, by removing consistently low counts in microbiome data sets [16, 17] or by removing near zero variance predictors. Such step aims to lessen the computational time during the parameter tuning process.

Multivariate projection-based methods

	- All multivariate approaches listed are projection-based methods whereby samples are summarised by H latent components or scores (t1, ..., tH) that are defined as linear combinations of the original predictors. In the combinations (t1, ..., tH), the weights of each of the predictors are indicated in the loading vectors as 1, ..., aH.

	- Unsupervised

		- Principal Component Analysis --- based on NonLinear Iterative Partial Least Squares for missing values [18]

		- Independent Component Analysis [19]

		- Partial Least Squares regression---PLS, also known as Projection to Latent Structures [20]

		- multi-group PLS [21]

		- regularised Canonical Correlation Analysis---rCCA [22])

		- regularised Generalised Canonical Correlation Analysis---rGCCA based on a PLS algorithm [23].

	- Supervised

		- PLS-Discriminant Analysis---PLS-DA [24--26]

		- GCC-DA [11]

		- multi-group PLS-DA [12]

### Partial Least Squares regression (PLS regression)

	- Partial least squares (PLS) regression is a technique that reduces the predictors to a smaller set of uncorrelated components and performs least squares regression on these components, instead of on the original data. PLS regression is especially useful when your predictors are highly collinear, or when you have more predictors than observations and ordinary least-squares regression either produces coefficients with high standard errors or fails completely. PLS does not assume that the predictors are fixed, unlike multiple regression. This means that the predictors can be measured with error, making PLS more robust to measurement uncertainty.

	- Unlike least squares regression, PLS can fit multiple response variables in a single model. PLS regression fits multiple response variables in a single model. Because PLS regression models the response variables in a multivariate way, the results can differ significantly from those calculated for the response variables individually.

	- is a statistical method that bears some relation to principal components regression

	- instead of finding hyperplanes of maximum variance between the response and independent variables, it finds a linear regression model by projecting the predicted variables and the observable variables to a new space. Because both the X and Y data are projected to new spaces, the PLS family of methods are known as bilinear factor models.

	- Partial least squares Discriminant Analysis (PLS-DA) is a variant used when the Y is categorical.

	- PLS is used to find the fundamental relations between two matrices (X and Y), i.e. a latent variable approach to modeling the covariance structures in these two spaces. A PLS model will try to find the multidimensional direction in the X space that explains the maximum multidimensional variance direction in the Y space.

	- PLS regression is particularly suited when the matrix of predictors has more variables than observations, and when there is multicollinearity among X values. By contrast, standard regression will fail in these cases (unless it is regularized).

	- Extensions:

		- Orthogonal Projections to Latent Structures (OPLS)

			- continuous variable data is separated into predictive ad uncorrelated information.

			- This leads to improved diagnostics, as well as more easily interpreted visualization.

			- However, these changes only improve the interpretability, not the predictivity, of the PLS models.

			- Similarly, OPLS-DA (Discriminant Analysis) may be applied when working with discrete variables, as in classification and biomarker studies.

	- Partial least squares regression extends multiple linear regression without imposing the restrictions employed by discriminant analysis, principal components regression, and canonical correlation.

	- Is probably the least restrictive of the various multivariate extensions of the multiple linear regression model. This flexibility allows it to be used in situations where the use of traditional multivariate methods is severely limited, such as when there are fewer observations than predictor variables.

	- Can be used as an exploratory analysis tool to select suitable predictor variables and to identify outliers before classical linear regression.

	Basic Model

		- As in multiple linear regression, the main purpose of partial least squares regression is to build a linear model, Y=XB+E, where Y is an n cases by m variables response matrix, X is an n cases by p variables predictor (design) matrix, B is a p by m regression coefficient matrix, and E is a noise term for the model which has the same dimensions as Y. Usually, the variables in X and Y are centered by subtracting their means and scaled by dividing by their standard deviations. For more information about centering and scaling in partial least squares regression, you can refer to Geladi and Kowalski(1986).

		- Both principal components regression and partial least squares regression produce factor scores as linear combinations of the original predictor variables, so that there is no correlation between the factor score variables used in the predictive regression model.

		- As in multiple linear regression, the main purpose of partial least squares regression is to build a linear model, Y=XB+E, where Y is an n cases by m variables response matrix, X is an n cases by p variables predictor (design) matrix, B is a p by m regression coefficient matrix, and E is a noise term for the model which has the same dimensions as Y. Usually, the variables in X and Y are centered by subtracting their means and scaled by dividing by their standard deviations. For more information about centering and scaling in partial least squares regression, you can refer to Geladi and Kowalski(1986).

		- Both principal components regression and partial least squares regression produce factor scores as linear combinations of the original predictor variables, so that there is no correlation between the factor score variables used in the predictive regression model

	Algorithms:

		- NIPALS algorithm

			- Herman Wold

			The standard algorithm for computing partial least squares regression components (i.e., factors) is nonlinear iterative partial least squares (NIPALS). There are many variants of the NIPALS algorithm which normalize or do not normalize certain vectors.

			- The algorithm reduces the number of predictors using a technique similar to principal components analysis to extract a set of components that describes maximum correlation between the predictors and response variables. PLS can calculate as many components as there are predictors; often, cross-validation is used to identify the smaller set of components that provide the greatest predictive ability. If you calculate all possible components, the resulting model is equivalent to the model you would obtain using least squares regression. In PLS, components are selected based on how much variance they explain in the predictors and between the predictors and the response(s). If the predictors are highly correlated, or if a smaller number of components perfectly model the response, then the number of components in the PLS model may be much less than the number of predictors. Minitab then performs least-squares regression on these uncorrelated components.

		- SIMPLS ALGORITHM

			- de Jong, 1993

### Variable selection for generalized canonical correlation analysis

Canonical correlation analysis (CCA) is a standard approach to studying the relationships between two blocks of variables only, and numerous L1 and/or L2 regularized extensions of the CCA have been proposed when the number of variables pj exceeds the number of observations n for any jth block (e.g. Vinod, 1976; Waaijenborg and others, 2008; Parkhomenko and others, 2009; Le Cao and others, 2009;

Witten and others, 2009; Lykou and Whittaker, 2010; Hardoon and Shawe-Taylor, 2011).

Regularized generalized canonical correlation analysis (RGCCA):

	- proposed in Tenenhaus and Tenenhaus (2011)

	- is a framework for studying associations between more than 2 blocks

	- The aim of RGCCA is to extract the information which is shared by the J blocks of variables taking into account an a priori graph of connections between blocks.

Biomedical data are known to be measurements of intrinsically parsimonious processes. In order to account for this parsimony and to improve the interpretability of the resulting RGCCA model, an important issue is to identify subsets of variables from each block which are active in the relation between connected blocks.

	- This variable selection step can be achieved by adding, within the RGCCA optimization problem, a penalty promoting sparsity. For that purpose, an L1 penalization on the outer weight vectors a 1, ..., a J is applied which induces a sparse RGCCA model that gives rise to sparse generalized canonical correlation analysis (SGCCA).

### Unravelling "omics" data with the mixOmics R package (ppt)

Issues with integrative systems biology

	- Unlimited quantity of data

	- n \<\< p problem

	- data from multiple sources

	-\> Efficient and biologically relevant statistical methodologies are needed to combine the information in heterogeneous data sets

Biological Questions:

	- Single Omics Analysis

		- do we observe a "natural" separation between the different groups of patients?

		- Can we identify potential biomarker candidates predicting the status of the patients?

	- Integrative Omics Analysis

		- Can we identify a subset of correlated genes and proteins from matching data sets?

		- Can we predict the abundance of a protein given the expression of a small subset of genes?

		- Do two matching omics data set contain the same information?

The data

	n = patients and p, q = features

	- Single Omics Analysis

		- one omic data set X(n x p)

		- for a supervised analysis, Y vector indicating the class of the patients

	- Integrative Omics Analysis

		- two matching omics data sets (measured on the same patients)

		- X (n x p) and Z (n x q)

Multivariate analysis:

	linear multivariate approaches enable:

		- Dimension reduction

		- to handle multicollinear, irrelevant, missing values

		- to capture experimental and biological variation

MixOmics

	- exploration and integrative analysis of high dimensional biological data sets

	- focus is on:

		- Data integration

		- Variable selection

		- Interpretable graphical outputs

	- The exploratory and integrative approaches are:

		- flexible and can answer various types of questions

		- can highlight the potential of the data

		- enable to generate new hypotheses to be further investigated

	Analysis:

		One data set

			Unsupervised:

				Multivariate approach: PCA \| IPCA

				Internal variable selection: sPCA \| sIPCA

				Graphical outputs: sample and variable plots

			Supervised:

				Multivariate approach: PLS-DA

				Internal variable selection: sPLS-DA

				Graphical outputs: sample and variable plots

		Two matching data sets

			canonical mode:

				Multivariate approach: PLS \| rCCA

				Internal variable selection: sPLS canonical

				Graphical outputs: sample and variable plots

			regression mode:

				Multivariate approach: PLS

				Internal variable selection: sPLS regression

				Graphical outputs: sample and variable plots

		Missing values

			imputation: NIPALS

	Future work:

		- Cross-platform comparison

		- Integration of multiple data sets (unsupervised and supervised)

		- Time-course experiments

Single Omics Analysis

	Principal Component Analysis (PCA)

		seek the best directions in the data that account for most of the variability

		-\> principal components: artificial variables that are linear combinations of the original variables: c = X v -\> (n) = (n x p ) (p)

			- c is a linear function of the elements of X having maximal variance

			- v is called the associated loading vector

		The new PCs form a vectorial subspace of dimension \< p

		- approximate representation of the data points in a lower dimensional space

		Problem: interpretation difficult with very large number of (possibly) irrelevant variables

		- unsupervised approach

	sparse Principal Component Analysis (sPCA)

		The principal components are linear combinations of the original variables, variables weights are defined in the associated loading vectors

		- computes the sparse loading vectors to remove irrelevant variables using lasso penalizations (Shen & Huang 2008, Multivariate Analysis)

	Independent Principal Component Analysis

		- assumes non Gaussian data distribution (!= PCA)

		- 'blind source' signal separation

		- seeks for a set of independent components (!= PCA)

		- combines the advantages of both PCA and ICA

		- the PCA loading are transformed via ICA to obtain independent loading vectors and independent principal components

		- sparse IPCA also developed to select the variable conributing to the independent loading vectors

		- Yao, F. Coquery, J. and Lê Cao, K-A. 2012 Independent Principal Component Analysis for biologically meaningful dimension reduction of large biological data sets, BMC Bioinformatics.

		- unsupervised approach

	PLS-Discriminant Analysis

		- Similarly to Linear Discriminant Analysis, classical PLS-DA looks for the best components to separate the sample groups

		- supervised approach

		- sPLS-DA searches for discriminative variables that can help separating the sample groups

		- evaluation of the discriminative power of the selected variables using external data sets or cross-validataion

		- Lê Cao K-A., Boitard S. and Besse P. (2011) Sparse PLS Discriminant Analysis: biologically relevant feature selection and graphical displays for multiclass problems, BMC Bioinformatics, 12:253.

Integrative Omics Analysis

	aims:

		- unravel the correlation structure between two data sets

		- select co-regulated biological entities across samples

		- Partial Least Squares regression maximises the covariance between each linear combination (components) associated to each data set

		- sparse PLS has been developed to include variable selection from both data sets

		- two modes are proposed to model the relationship between the two data sets: regression and canonical

	- sPLS:

		Sample plot:

			- aims at selecting correlated variables across the same samples by performing a multivariate regression

			- regression: explain the protein abundance w. r. t. the gene expression relationship

			- the latent variables (components) are determined based on the selected genes and proteins -\> give more insight into the samples similarities

			- unsupervised approach

		Variable plot:

			- relevance networks are bipartite graphs directly inferred from the sPLS components

			- other insightful graphical outputs:

				- correlation circle plots

				- clustered image maps

			- González I., Lê Cao K.-A., Davis, M.D. and Déjean S. Visualising association between paired \`omics' data sets. In revision.

		- canonical mode:

			- selects correlated variables across the same samples and highlights the correlation structure between the two data sets

		- Arrow plot: highlight the similarities between 2 data sets.

Cross-over desing

	One data set repeated measurements (n x p)

		supervised

			1 level

				Multivariate approach

					multilevel PLS-DA

				Internal variable selection

					multilevel sPLS-DA

				Graphical outputs: sample and variable plots

			2 levels

				Multivariate approach

					multilevel PLS-DA

				Internal variable selection

					multilevel sPLS-DA

				Graphical outputs: sample and variable plots

	Two matching data sets repeated measurements (n x p) & (n x q)

		canonical, 1 level

			Multivariate approach

				multilevel PLS

			Internal variable selection

				multilevel sPLS

			Graphical outputs: sample and variable plots

PCA

sPCA - sparse Principal Component Analysis

ICA - Independent Component Analysis

IPCA - Independent Principal Component Analysis

	Yao, F. Coquery, J. and Lê Cao, K-A. 2012 Independent Principal Component Analysis for biologically meaningful dimension reduction of large biological data sets, BMC Bioinformatics.

LDA: Linear Discriminant Analysis

PLS-DA: PLS - Discriminant Analysis

sPLS-DA: sparse PLS Discriminant Analysis

	Lê Cao K-A., Boitard S. and Besse P. (2011) Sparse PLS Discriminant Analysis: biologically relevant feature selection and graphical displays for multiclass problems, BMC Bioinformatics, 12:253.

Graphical methods:

	- sample plot

	- variable plot

	- correlation circle plots

	- clustered image maps

to know:

## Normalization

### The Universal exPression Code (UPC) algorithm consists of two main steps:

	i. for each platform, linear statistical models correct for background noise by modeling the genomic base composition and length of target regions;

	ii. estimates of transcriptional activation are calculated using a two-component mixture model, which assumes that background expression levels should be similar for genes having similar molecular characteristics.

QUESTIONS:

	1. how do you correct something using linear models?

	2. how the genomic base composition and lenght of target regions are incorporated in the correction?

	3. what are two-component mixture model?

Let Yi denote the unnormalized expression measurement for gene i.

Assume that

	$Y_i = (1 - \Delta_i) Y_{1i} + \Delta_i Y_{2i}$,

where:

	- $Y_{1i}$ = random variable from the 'background' distribution for the gene;

	- $Y_{2i}$ originates from the "background-plus-signal" distribution

	- $\Delta_i$ is an unobserved indicator variable that is equal to 1 if gene i is active and 0 otherwise;

QUESTIONS:

	- what is the expectation-maximization (EM) algorithm?

The UPC value for gene i, denoted Pi, is given by the expected value of DELTAi, given that the parameters pi,

## Time-Series Analysis

#### Basics Time Series Analysis in R

	- <http://luthuli.cs.uiuc.edu/~daf/courses/cs-498-daf-ps/lecture%2018%20-%20time%20series,%20dtw.pdf>

	- <https://www.datacamp.com/community/tutorials/time-series-r>

	- <https://www.analyticsvidhya.com/blog/2015/12/complete-tutorial-time-series-modeling/>

	- <https://www.stat.pitt.edu/stoffer/tsa4/R_toot.htm>

	- <https://nwfsc-timeseries.github.io/atsa-labs/sec-ts-time-series-plots.html>	

#### Time Series Similarity Measures

DTW (Dynamic Time Warping)

	- <http://www.phon.ox.ac.uk/jcoleman/old_SLP/Lecture_5/DTW_explanation.html>

	- <https://pdfs.semanticscholar.org/57e1/704fd41ac85f57e68d75576645d7496c4e55.pdf>

	- <http://seninp.github.io/assets/pubs/senin_dtw_litreview_2008.pdf>

	- <https://cran.r-project.org/web/packages/dtwclust/vignettes/timing-experiments.html>

	- <https://a-little-book-of-r-for-time-series.readthedocs.io/en/latest/src/timeseries.html>

MIC (Maximal Information Coefficient)

DTW-MIC

Time Series Similarity Measure Comparison

	- <http://www.earthmapps.io/pubs/2011_Lhermitte_A%20comparison%20of%20time%20series.pdf>

### Time-Series Clustering

Cluster analysis is a task which concerns itself with the creation of groups of objects, where each group is called a cluster. Ideally, all members of the same cluster are similar to each other, but are as dissimilar as possible from objects in a different cluster.

For static data, clustering methods are usually divided into:

	- partitioning

	- hierarchical

	- density-based

	- grid-based

	- model-based

In the context of time-series, dimensionality of a series is related to time, and it can be understood as the length of the series.

	então quando eu faço o calpsamento por idade, estou reduzindo a dimensionalidade

A single time-series object may be consituted of several values that change on the same time scale, in which case they are identified as multivariate time-series.

There are many techniques to modify time-series in order to reduce dimensionality, and they mostly deal with the way time-series are represented.

Time-series clustering is a type of clustering algorithm made to handle dynamic data.

	most important elements to consider:

	- (dis)similarity or distance measure

	- prototype extraction function (if applicable)

	- clustering algorithm

	- cluster evaluation

In most cases, algorithms developed for time-series clustering take static clustering algorithms and either modify the similarity definition or the prototype extraction function to an appropriate one, or apply a transformation to the series so that static features are obtained. Therefore, the underlying basis for the different clustering procedures remains approximately the same across clustering methods.

The most common clustering methods for time-series are:

	- hierarchical

	- partitional and fuzzy

Classification of time-series clustering algorithms by Aghabozorgi et al. (2015), based on the way they treat the data and how the underlying grouping is performed:

	1. whether the whole series, a subsequence, or individual time points are to be clustered.

	2. if the clustering itself may be shape-based, feature-based or model-based.

In this context, it is common to utilize the Dynamic Time Warping (DTW) distance as dissimilarity measure. The calculation of the DTW distance involves a dynamic programming algorithm that tries to find the optimum warping path between two series under certain constraints.

	DISTANCE MEASURES

	1. Dynamic Time Warping - DTW

	2. Global Alignment Kernel - GAK

	3. Shape-Based Distance - SBD

	TIME-SERIES PROTOTYPES

	1. Mean and median

	2. Partition around medoids

	3. DTW barycenter averaging

	4. Shape extraction

	5. Soft-DTW centroid

	6. Fuzzy-based prototypes

	TIME-SERIES CLUSTERING ALGORITHMS

	1. Hierarchical clustering

	2. Partitional clustering

		2.1 TADPole clustering

		2.2 k-Shaped clustering

	3. Fuzzy clustering	

	CLUSTER EVALUATION

		- cluster validity indices (CVIs)

		- Arbelaitz et al.(2013) and Wang and Zhang (2007);

		CVIs can be either tailored to crisp or fuzzy partitions.

		crisp:

			- internal, external or relative

### Time-Series Options

	- linear models

		Linear models and empirical bayes methods for assessing differential expression in microarray experiments

	- empirical Bayes

		A multivariate empirical Bayes statistic for replicated microarray time course data

	- fuzzy algorithms

		Mfuzz: a software package for soft clustering of microarray data.

	- Bayesian approaches

		An improved empirical bayes approach to estimating differential gene expression in microarray time-course data: BETR (Bayesian Estimation of Temporal Regulation)

Desktop Java Applications:

	- Short Time-series Expression Miner (STEM)

		STEM: a tool for the analysis of short time series gene expression data

	- Bayesian Analysis of Time Series (BATS)

		BATS: a Bayesian user-friendly software for analyzing time series microarray experiments.

	- GenTχWarper

		Gene Time Expression Warper: a tool for alignment, template matching and visualization of gene expression time series

	- EDGE

		EDGE: extraction and analysis of differential gene expression.

\- regression-based (maSigPro)

	maSigPro: a method to identify significantly differential expression profiles in time-course microarray experiments.

\- multivariate approaches (ASCA-genes)

	Discovering gene expression patterns in time course microarray experiments by ANOVA-SCA

\- specific methodologies for functional and gene-set enrichment analysis (maSigFun, PCA-maSigFun and ASCA-functional)

	Functional assessment of time course microarray data

\- Serial Expression Analysis (SEA)

About time-series:

time series data are often mathematically nonstationary and show autocorrelation, creating nonlinearities or discontinuities in the data, which limit the use of many statistical techniques that assume fixed or Gaussian probability distributions (47).

	47.

A further complication of analyzing time series data is that expression patterns in pairs of genes are often not monotonically correlated, rendering inappropriate commonly used nonparametric tests of association, such as the Spearman rank correlation (48, 49).

	48.

	49.

Time-series analysis methods:

	- Dynamic Time Warping

	- Independent Component Analysis

	- Mutual information-based network analysis

	Dynamic Time Warping (DTW)

		DTW creates this distance measure by locally compressing or stretching (warping) one trace to best match the other and then summing the distances of individual aligned elements.

		

		DTW provides high power to detect differences in time series by accounting for the trajectory of changes in expression over time, unlike a purely statistical technique, such as analysis of covariance (ANCOVA).

		DTW distance increased linearly with random noise and in proportion to the length of random traces being compared.Statistical strategies include:

	- linear models

		Linear models and empirical bayes methods for assessing differential expression in microarray experiments

	- empirical Bayes

		A multivariate empirical Bayes statistic for replicated microarray time course data

	- fuzzy algorithms

		Mfuzz: a software package for soft clustering of microarray data.

	- Bayesian approaches

		An improved empirical bayes approach to estimating differential gene expression in microarray time-course data: BETR (Bayesian Estimation of Temporal Regulation)

Desktop Java Applications:

	- Short Time-series Expression Miner (STEM)

		STEM: a tool for the analysis of short time series gene expression data

	- Bayesian Analysis of Time Series (BATS)

		BATS: a Bayesian user-friendly software for analyzing time series microarray experiments.

	- GenTχWarper

		Gene Time Expression Warper: a tool for alignment, template matching and visualization of gene expression time series

	- EDGE

		EDGE: extraction and analysis of differential gene expression.

\- regression-based (maSigPro)

	maSigPro: a method to identify significantly differential expression profiles in time-course microarray experiments.

\- multivariate approaches (ASCA-genes)

	Discovering gene expression patterns in time course microarray experiments by ANOVA-SCA

\- specific methodologies for functional and gene-set enrichment analysis (maSigFun, PCA-maSigFun and ASCA-functional)

	Functional assessment of time course microarray data

\- Serial Expression Analysis (SEA)

About time-series:

time series data are often mathematically nonstationary and show autocorrelation, creating nonlinearities or discontinuities in the data, which limit the use of many statistical techniques that assume fixed or Gaussian probability distributions (47).

	47.

A further complication of analyzing time series data is that expression patterns in pairs of genes are often not monotonically correlated, rendering inappropriate commonly used nonparametric tests of association, such as the Spearman rank correlation (48, 49).

	48.

	49.

Time-series analysis methods:

	- Dynamic Time Warping

	- Independent Component Analysis

	- Mutual information-based network analysis

	Dynamic Time Warping (DTW)

		DTW creates this distance measure by locally compressing or stretching (warping) one trace to best match the other and then summing the distances of individual aligned elements.

		

		DTW provides high power to detect differences in time series by accounting for the trajectory of changes in expression over time, unlike a purely statistical technique, such as analysis of covariance (ANCOVA).

		DTW distance increased linearly with random noise and in proportion to the length of random traces being compared.

## Survival analysis

<https://www.youtube.com/watch?v=fTX8GghbBPc&list=PLRW9kMvtNZOjXOgq4XTBTqdLb89YW1Ygc>

ALIAS: duration/transition/failure time/time-to-event analysis

Survival analysis set up

	- subjects are tracked until an event happens (failure) or wwe lose then frin the sample (censored observationss)

	- we are interested in how long they stay in the sample (survival)

	- we are also interested in their risk of failure (hazard rates)

Survival analysis features

	- the dependent variable is duration (time to event or time to being censored) so it is a combination of time and event/censoring

		- time variable = length of time until the event happended or as lond as they are in the study

		- the event variable: 1 if the event happened or 0 if the event has not yet happened

		- instead of an event variable, a censor variable can be defined The censored variabl = if the event has not happened yet, and - if the event has happened

	- hazard rate: is the probability that the event will happen at time t given that the individual is at risk at time t.

	- Hazard rates usually change over time.

		- the probability of defaulting on a loan may be low in the beggining but increases over the time

Extensions of the basic survival analysis

	+ Multiple occurrences of event (mlutiple observations per individual)

		- borrower may have repeated restructruing of the loan

		- firm may adoip techonlogy in some year but not others

	+ More than one type of event (include codes for events e.g., 1, 2,3,4

		- borrower may default (one tyep of event) the loan earlier (a secont type of event)

		= firms may adopt different types of technologies

	+Two groups of participants

		- the effect of two types of educational programs on techonology adoption rates

	+ Time-varying covariates

		- borrower\`s income may have changed during the study which caused the default.

	 + Discrete instead of continupous transition times

	 	- exists are measured in intervals (such as every month)

	 + there may different starting times - weneed to measue time from the beggingin time to ehe event

SURVIVAL, HAZARD, AND CUMULATIVE HAZARD FUNCTIONS

	+ the dependent variable duration is assumed to fave a continuous probability dist f(t)

	+ the probability that the duratoin time will be less than t is:

		F(t) = Prob(T \<= t) = Integra [0 -\> t] f(s)ds

	+ Survival function is the probability that the duration will be at least t:

		S(t) = 1 -F(t) = Prob(T \>= t)

	+ Survival function is the probability that the duration will be at least t:

		S(t) = 1 - F(t) = Prob(T \>= t)

	+ Hazard rate is the probability that the duration will end after time t, given that it has lasted ultil time t:

		lambda(t) = f(t)/S(t)

	+ the hazard rate is the probability that an individual will experience the event at time t while that individua is at risk for experiencing the event

NONPARAMETRIC MODELS

	nonparametric estimation is useful for descriptive purposes and to see the shape of the hazard or survival function before a pranatric model with regressors is tintroduced

	- Think about the shapes of the hazard funcoin an dsurvival function plotter over time



source: <https://www.datacamp.com/community/tutorials/survival-analysis-R>

In this type of analysis (Survival Analysis) the time to a specific event is of interest and two (or more) groups of patients are compared with respect to this time.

Three core concepts can be used to derive meaningul results from such a dataset:

	+ The Statistics behind Survival Analysis

		- Kaplan-Meier Method and Log-Rank Test

		- Cox Proportional Hazards Models

This type of analysis can answer questions such as the following:

	- do patients benefit from therapy regimen A as opposed to regimen B?

	- Do patients\` age and fitness significantly influence the outcome?

	- Is residual disease a prognostic biomarker in terms of survival?

Terminology:

	- event: is the pre-specified endpoint of your study (e.g. death, disease recurrence)

	- censoring: refers to incomplete data. All patientes who do not experience the "event" until the study ends will be censored at that last time point

		- right-censoring

			- fixed or random type I censoring

			- type II censoring

	- Covariates: aka explanatory or independent variables in regression analysis, are variables that are possibly predictive of an outcome or that you might want to adjust for to account for interactions between variables.

Statistics:

	- Kaplan-Meirer Method and Log Rank Test

	Kaplan-Meir estimator:

		0 independently described by Edward Kaplan and Paul Meier, anc conjointly published in 1958 in the Journal of the Aerican Statistical Association

		- is a non-parametric statistic

		- allows us to estimate the survival function

		OBS: makes sense to use non-parametric statistic since survival data has a skewed distribution

		- this statistic gives the probability that an individual patient will survive past a particular time t.

			- at t = 0, the Kaplan-Meier estimator is 1 and with t going to infinity, the estimator goes to 0.

			- in theory, with an infinetely large dataset and t measured to the second, the correponding functon t versus survival probaility is smooth.

		- assumption: the probability of surviving past a certin time point t is equal to the product of the oserved survival rates until time point t:

			- S(t) = p.1 \* p.2 \* ... \* p.t, with:

				S(t) = the survival probability at time t

				p.1 = proportion of all patients surviving past the first time point

				p.2 = proportion of all patients surviving past the second time point

				... an so forth until time point t is reahed.

		- IMPORTANT: starting w/ p.2 and up to p.t, you take only those patients into account who survived past the previous time point when calculating the proportions for every next time point

			=\> thus, p.2, p.3, ..., p.t are proportions that are conditional on the previous proportions

		- in practice, you want to organize the survival times in order of increasing duration first. This includes the censored values. You then want to calculate the proportions as described above and sum them up to derive S(t).

			Censored patients are omitted after the time point of censoring, so they do not influence the proportion of surviving patients.

	- Log-rank test:

		- Can use to compare survival curves of two groups.

		- is a statistical hypothesis test that tests the null hypothesis that survival curves of two populations do not differ.

		- a certain probability distribution, namely a chi-squared distribution, can be used to derive a p-value.

		- compares two Kaplan-Meier survival curves, which might be derived from splitting a patient population into treatment subgroups

	- Cox Proportional Hazards Models

		- h(t): hazard function

		- describes the probability of an event or its hazard h (survival in this case) if the subject survived up to that particular time point t.

		- it measures the instantaneous risk of death

		- you need the hazard function to consider covariates when you compare survival of patient groups.

		- are derived from the underlying baseline hazard functions of the patinent populations in question and an arbitrary number of dichotomized covariates.

		- does not assume an underlying probability distribution but it assumes that the hazards of the patient groups you compare are constant over time -\> that is why is called "proportional hazards model"

		- allow to include covariates

		- forest plot. It shows so-called hazard ratios (HR) which are derived from the model for all covariates that we included in the formula

		- HR \> 1 indicates an increased risk of death (according to the definition of h(t)) if a specific condition is met by a patient. An HR \< 1, on the other hand, indicates a decreased risk.

## Data Integration

### Sparse canonical methods for biological data integration: application to a cross-platform study

ABSTRACT

	Results:

		We compare the results obtained with two other sparse or related canonical correlation approaches:

			- CCA with Elastic Net penalization (CCA-EN)

			- Co-Inertia Analysis (CIA)

				- does not include a built-in procedure for variable selection

				- requires a two-step analysis

		- There is a lack of statistical criteria to evaluate canonical correlation methods, which makes biological interpretation absolutely necessary to compare the different gene selections

	Conclusions:

		- sPLS and CCA-EN selected highly relevante genes and complementary findings from the two datasets

		- These two approaches were found to bring similar results, athough they highlighted the same phenomenons with a different priority

		- They outperformed CIA that tended to select redundant information

BACKGROUND

	Few approaches exists to deal with high-throughput data sets:

	linear multivariate models:

		Partial Least Squares regression (PLS,[1])

		Canonical Correlation Analysis (CCA, [2])

		Problems:

		- are often limited by the size of the data set (ill-posed problems, CCA)

		- the noisy and the multicollinearity characteristics of the data (CCA)

		- lack of interpretability (PLS)

		- PLS has often been criticized for its lack of theoretical justifications. Much work still needs to be done to demonstrate all statistical properties of the PLS

		Advantages:

			1. because they allow for the compression of the data into 2 to 3 dimensions for a more powerful and global view

			2. because their resulting components and loading vectors capture dominant and latent properties of the studied process. They may hence provide a better understanding of the underlying biological systems, for example by revealing groups of samples that were previously unknown or uncertain.

		Canonical correlation framework

			- there is either no assumption no the relationship between the two sets of variables (exploratory approach), or when a reciprocal reelationship between the two sets is expected (e.g., cross platform comparions).

			- When applying canonical correlation-based methods, most validation criteria used in a regression context are not statistically meaningful. Instead, the biological relevancy of the results should be evaluated during the validation process.

			- Some sparse associated integrative approaches have recently been developed to include a built-in selection procedure. They adapt lasso penalty [7] or combine lasso and ridge penalties (Elastic Net, [8]) for feature selection in integration studies.

			- Approaches:

				- penalized CCA adapted w/ Elastic Net (CCA-EN[10])

				- Co-Inertia Anaysis (CIA[11])

	This study propose to apply a sparse canonical approach callsed "sparse PLS" (sPLS) for the integration of high throughtput data sets.

		- provides variable selection of two-block data sets in a one step procedure, while integrating variables of two types

		- Methodological aspects and evaluation of sPLS in a regression framework:

			[9]

	Canonical correlation-based methods

		focus on two-block data matrices: X(n x p) and Y (n x q), where p and q are of two types, measured on the same samples (n)

		- Prior biological knowledge on these data allows us to settles into a canonical framework, i.e., there exists a reciprocal relationship between the X variables and the Y variables

		- the large number of variables may affect the exploratory method, due to numerical issues (e.g., CCA) or lack of interpretability (PLS)

		Three types of multivariate methods: CCA, PLS, CIA

		CCA

			Canonical Correlation Analysis

			- studies the relationship between two sets of data

			- the CCA n-dimensional score vectors (Xah, Ybh) come in pair to solve the objective function arg max cor(Xah, Ybh)

			- the aim of CCA is to simultaneously maximize cov(Xah, Ybh) and minimize the variances of Xah and Ybh.

		PLS

			- Partial Least Squares regression

			- based on the simultaneous decomposition of X and Y into latent variables and associated loading vectors.

			- The latent variables methods (e.g. PLS, Principal Component Regression) assume that the studied system is driven by a small number of n-dimensional vectors called latent variables.

			- These latter may correspond to some biological underlying phenomenons which are related to the study [17].

			- Like CCA, the PLS latent variables are linear combinations of the variables, but the objective function differs as it is based on the maximization of the covariance

			- In contrary to CCA, the loading vectors (a h , b h ) are interpretable and can give information about how the x j and y k variables combine to explain the relationships between X and Y. Furthermore, the PLS latent variables ( h , h ) indicate the similarities or dissimilarities between the individuals, related to the loading vectors

			- Many PLS algorithms exist:

				- for different shapes of data (SIMPLS, [18], PLS1 and PLS2 [1], PLS-SVD [19])

				- different aims:

					- predictive, like PLS2

					- modelling, like PLS-mode A, see [10,20,21]

			- In this study we especially focus on a modelling aim ("canonical mode") between the two data sets, by deflating X and Y in a symmetric way

		CCA-EN

			- proposed by [10]

			- sparse penalized variant of CCA using Elastic Net [8,22] for a canonical framework

			- Elastic Net: combines the advantages of the ridge regression, that penalizes the covariance matrices XX' and YY' which become non singular, and the lasso [7] that allows variable selection, in a one step procedure.

			- However, when p + q is very large, the resolution of the optimization problem requires intensive computations, and [8,10] proposed instead to perform a univariate thresholding, that leaves only the lasso estimates to compute

		sparse PLS

			- proposed by [9]

			- sparse PLS approach (sPLS) based on a PLS-SVD variant, so as to penalize both loading vectors ah and bh simultaneously.

			- parsity can then be introduced by iteratively penalizing ah and bh with a soft-thresholding penalization, as [23] proposed for a sparse PCA using SVD computation. Both regression and canonical deflation modes were proposed for sPLS [9]

		CIA

			- Co-Inertia analysis

			- introduced by [11], applied to ecological data

			- first application to biological data [12]

			- suitable for a canonical framework, as it is adapted for a symmetric analysis.

			- It involves analyzing each data set separately either with principal component analyses, or with correspondence analyses, such that the covariance between the two new sets of projected scores vectors (that maximize either the projected variability or inertia) is maximal.

			- This results in two sets of axes, where the first pair of axes are maximally co-variant, and are orthogonal to the next pair [24]. CIA does not propose a built-in variable selection, but we can perform instead a two-step procedure by ordering the weight vector (loadings) for each CIA dimension and by selecting the top variables.

		Differences between the approaches

			- profoundly differ in ther construction and aims

			- CCA-EN looks for canonical variate pairs (Xa h , Yb h ), such that a penalized version of the canonical correlation is maximized.

				- This explains why a non monotonic decreasing trend in the canonical correlation can sometimes be obtained [10].

			- On the other hand, sPLS (canonical mode) and CIA aim at maximizing the covariance between the scores vectors, so that there is a strong symmetric relationship between both sets.

			- However, here CIA is based on the construction of two Correspondence Analyses, whereas sPLS is based on a PLS analysis.

		Parameters tuning

			- In CCA-EN, the authors proposed to tune the penalty parameters for each dimension, such that the canonical correlation cor(Xa h , Yb h ) is maximized.

				- In practice, they showed that the correlation did not change much when more variables were added in the selection.

				- Therefore, an appropriate way of tuning the parameters would be to choose instead the degree of sparsity (i.e. the number of variables to select), as previously proposed for sparse PCA by [22,23]-see the elasticnet R package for example, and hence to rely on the biologists needs.

					- Thus, depending on the aim of the study (focus on few genes or on groups of genes such as whole pathways) and on the ability to perform follow-up studies, the size of the selection can be adapted.

						- When focusing on groups of genes (e.g. pathways, transcription factor targets, variables involved in the same biological process), we believe that the selection should be large enough to avoid missing specific functions or annotations.

					- The same strategy will be used for sPLS (see also [9] where the issue of tuning sPLS parameters is addressed).

				- No other parameters than the number of selected variables is needed in CIA either

		Outputs

			Samples

				Samples are represented with the scores or latent variable vectors, in a superimposed manner, as proposed in the R package ade4 [25]:

					1. show how samples are clustered, based on their biological characteristics

					2. measure if both data sets strongly agree according to the applied approach

				- each sample is indicated using an arrow.

					- The start of the arrow indicates the location of the sample in the X data set in one plot, and the tip of the arrow the location of the sample in the Y data set in the other plot.

					- Thus, short (long) arrows indicate if both data sets strongly agree (disagree) between the two data sets.

			Variables

				Variables are represented on correlation circles, as previously proposed by [14]

				Correlations between the original data sets and the score or latent variable vectors are computed so that highly correlated variables cluster together in the resulting graphics.

				Only the selected variables in each dimension are represented.

				This type of graphic not only allows for the identification of interactions between the two types of variables, but also for identifying the relationship between variable clusters and associated sample clusters.

	Cross-platform study

		Data sets and relevance for acanonical correlation analysis

		The Ross Data Set

		The Staunton Data set

		Application of the three sparse canonical correlation-based methods

RESULTS AND DISCUSSIONS

	How to assess the results?

		- Canonical correlation-based methods are statistically difficult to assess\>

			1. they do not fit into a regression/prediction framework, meaning that the prediction error cannot be estimated using cross-validation to evaluate the quality of the model

			2. because in many two-block biological studies, the number of samples n is very small compared to the number of variables p + q. This makes any statistical criteria difficult to compute or estimate.

		- This is why graphical outputs are important to help analyze the results

	Link between two-block data sets

		Variance explained by each component

		Correlations between each component

	Interpretation of the observed cell line clusters

		Graphical representation of the samples

		Hierarchical clustering of the samples

	Interpretation of the observed genes clusters

		Graphical representation of the genes

		Analysis of the gene lists

		Analysis of the gene lists with IPA

			Over-represented biological functions

			Canonical pathways

			Networks

CONCLUSION

	CIA

		CIA does not propose a built-in variable selection procedure and requires a two-step analysis to perform variable selection. The main individual effects were identified.

		However, the loadings or weight vectors obtained were not orthogonal, in contrary to CCA-EN and sPLS. This resulted in some redundancy in the gene selections, which may be a limitation for the biological interpretation, as it led to less specific results.

	CCA-EN

		CCA-EN first captured the main robust effect on the individuals that was present in the two data sets. As a consequence, it may hide strongest individual effects that are present in only one data set. We observed a strong similarity between CCA-EN and sPLS in the gene selections, except that the first two axes were permuted. In fact, we believe that CCA-EN can be considered as a sparse PLS variant with a canonical mode. Indeed, the elastic net is approximated with a univariate threshold, which is similar to a lasso soft-thresholding penalization, and the whole algorithm uses PLS and not CCA computations.

		This explains why the canonical correlations do not monotonically decrease. The only difference that distinguishes sPLS canonical mode from CCA-EN is the initialization of the algorithm for each dimension. CCA-EN maximizes the correlation between the latent variables, whereas sPLS maximizes the covariance.

	sPLS

		We found that sPLS made a good compromise between all these approaches. It includes variable selection and the loading vectors are orthogonal. Although sPLS and CCA-EN do not order the axis in the same manner, both approaches were highly similar, except for slight but significant differences when studying LE vs. CO (Set 3). In this particular case, the resulting gene lists clearly provided complementary information.

Based on the present study, we would primarily recommend the use of CCA-EN or sPLS when gene selection is an issue. Like CCA-EN, sPLS includes a built-in variable

selection procedure but captured subtle individual effects. Therefore, these two approaches may differ when computing the fist axes. All approaches are easy to use and fast to compute. These approaches would benefit from the development of an R package to harmonize their inputs and outputs so as to facilitate their use and their comparison.

## Single-Cell RNA-Seq

<https://hemberg-lab.github.io/scRNA.seq.course/biological-analysis.html#pseudotime-analysis>

### Analysis

single-cell RNA-Seq
	PACKAGES:
		- Conos (Clustering on Network of Samples)
			+ https://github.com/hms-dbmi/conos
			- It's a package to wire together large collections of single-cell RNA-seq datasets.
			- It focuses on uniform mapping of homologous cell types across heterogeneous sample collections.
			
		- SCDE
			+ http://hms-dbmi.github.io/scde/index.html
			- implements a set of statistical methods for analyzing single-cell RNA-seq data, including differential expression analysis and pathway and geneset overdispersion analysis
			- Single cell error modeling
				fits individual error models for single cells using counts derived from single-cell RNA-seq data to estimate drop-out and amplification biases on gene expression magnitude.

			- Differential expression analysis
				compares groups of single cells and tests for differential expression, taking into account variability in the single cell RNA-seq data due to drop-out and amplification biases in order to identify more robustly differentially expressed genes.

			- Pathway and gene set overdispersion analysis
				contains pagoda routines that characterize aspects of transcriptional heterogeneity in populations of single cells using pre-defined gene sets as well as 'de novo' gene sets derived from the data. Significant aspects are used to cluster cells into subpopulations. A graphical user interface can be deployed to interactively explore results.

		- PAGODA
			- https://github.com/hms-dbmi/pagoda2
			- http://pklab.med.harvard.edu/nikolas/pagoda2/frontend/current/pagodaURL/index.html?fileURL=http://pklab.med.harvard.edu/nikolas/pagoda2/staticDemo/pagodaPublicDemo.bin
			- http://pklab.med.harvard.edu/scde/pagoda.links.html

		- VELOCYTO
			+ http://velocyto.org/
			- analysis of expression dynamics in single cell RNA seq data.
			- enables estimations of RNA velocities of single cells by distinguishing unspliced and spliced mRNAs in standard single-cell RNA sequencing protocols
			- http://velocyto.org/velocyto.py/tutorial/cli.html


	TUTORIALS:
		https://hemberg-lab.github.io/scRNA.seq.course/cleaning-the-expression-matrix.html

## Reproducible Data Science

Reproducible Data Science w/ R

	<https://resources.rstudio.com/rstudio-conf-2019/a-guide-to-modern-reproducible-data-science-with-r>

	Research compendia

		"...We introduce the concept of a compendium as both a container for the different elements that make up the document and its computations (i.e. text, code, data, ...), and as a means for distributing, managing and updating the collection."

	Research compendium principles

		- stick with the conventions of your peers

		- Keep data, methods and outputs separate

		- Specify your computational environment as clear as you can

	Key components you\`ll need for sharing a compendium:

		License + VCS + Metadata + Archive

	compendium DESCRIPTION file

		Type: Compendium

		Package: pomdpintro

		Version: 0.1.0

		Depends: nimble, tidyverse, sarsop, MDPtoolbox

		Suggests: extrafont, hrbrthemes, Cairo, ggthemes

		Remotes: boettiger-lab/sarsop

	Packaging your analysis as a compendium gives you access to powerfull developer tools

	Small compendia

		COMPENDIUM

			\|

			\|---	DESCRIPTION

			\|

			\|---	LICENSE

			\|

			\|---	Readme.md

			\|

			\|---	data/

			\|		 \|

			\|		 '--------- Mydata.csv

			\|

			'---	analysis/

					 \|

					 '---	Report.Rmd

	Components of a compendium

		Data (Small -\> Medium)

			How does one manage small to medium data in the context of a research compendium?

				Small data

					Put small data inside packages, especially if you ship a methods package with your analysis

					CRAN = \< 5 mb

					37% of the 13K packages on CRAN have some form of data.

					Leveraging Github releases to share medium sized files

						piggyack

							attach large [data] files to Github repositories

							github.com/ropensci/piggyback

							

							pb_new_release('user/repo', 'v0.0.5')

							pb_upload('datasets.tsv.xz','user/repo')

							Access them in your scripts with

							pb_download

				Medium data

					github.com/ropensci/arkdb

		Computing environment

			Its important to isolate the computing environment so that changes in software dependencies don\`t break your analysis

			Adding a Dockerfile to your compendium

				Many ways to write a Dockerfile for your R project

				o2r/containerit

			Binder

				mybinder.org

				Binder is an open source project that is designed to make it really easy to share analkysis that are in notebooks

				Git + Docker + RStudio

		

		Workflows

			Include a workflow to manage relationships between data output and code

			drake

				general purpose workflow manager & pipeline toolkit for reproducibility and high-performance computing

				github.com/ropensci/drake

				No cumbersome Makefiles

				Vast arsenal of parallel computing options

				Visualize dependency graph and estimate run times

				Convenient organization of output

RESEARCH COMPENDIUM

	<https://research-compendium.science>

	<https://github.com/research-compendium/research-compendium.github.io>

	<http://inundata.org/talks/rstd19/#/>

	<https://github.com/karthik/rstudio2019>

	<https://biostats.bepress.com/cgi/viewcontent.cgi?article=1001&context=bioconductor>

## machine learning vs DE Analysis

		<https://www.biostars.org/p/305532/>

## Dealing with Missing Data

MISSING DATA

	- Resources

		<https://www.analyticsvidhya.com/blog/2016/03/tutorial-powerful-packages-imputing-missing-values/>

	- DATACAMP

		- "The best thing to do with missing data is to not have any" - Gertrude Mary Cox

		- Missing data can have unexpected effects on your analysis

		- Bad imputation can lead to poor estimates and decisions

		- missing data = NA, Not Available

		- R consider NaN (Not a Number) as NA

		- missingness summaries

			- any_na(x)

			- are_na(x)

			- n_miss(x)

			- prop_miss(x)

			

			- basic summaries:

				- n_miss

				- n_complete

			- dataframe summaries:

				- miss_var_summary

				- miss_case_summary

		- missing data tabulations

			- miss_var_table

			- miss_case_table

		- miss_var_span()

		- miss_var_run() find repeated patterns of missing data in a run

		Visualizations

			- viss_miss()

			- gg_miss_var()

			- gg_miss_case()

			- gg_miss_upset()

			- gg_miss_fct()

			- gg_miss_span()

## Feature Selection

### Features and feature engineering

The goals of a good feature are to simultaneously vary with what matters and be
invariant with what does not.

A natural question is whether or not we can select good features automatically.
This problem is known as feature selection. There are many methods that have
been proposed for this problem, but in practice, very simple ideas work best. It
does not make sense to use feature selection in these small problems, but if you
had thousands of features, throwing out most of them might make the rest of the
process much faster.

spectral feature selection r

<https://cran.r-project.org/web/packages/sparcl/index.html>

<https://www.researchgate.net/post/What_is_the_best_unsupervised_method_for_feature_subset_selection>

<https://stats.stackexchange.com/questions/108743/methods-in-r-or-python-to-perform-feature-selection-in-unsupervised-learning>

<https://www.google.co.uk/search?client=opera&q=unsupervised+variable+selection+r&sourceid=opera&ie=UTF-8&oe=UTF-8>

<https://www.datacamp.com/community/tutorials/introduction-t-sne>

<https://cran.r-project.org/web/packages/tsna/vignettes/tsna_vignette.html>

## Exploratory Data Anaysis (EDA)

Iterative cycle:

	1. Generate questions about your data

	2. Search for answers by visualising, transforming, and modelling your data

	3. Use what you learn to refine your questions and/or generate new questions

Your goal during EDA is to develop an understanding of your data. The easiest way to do this is to use questions as tools to guide your investigation. When you ask a question, the question focuses your attention on a specific part of your dataset and helps you decide which graphs, models, or transformations to make.

EDA is fundamentally a creative process. And like most creative processes, the key to asking quality questions is to generate a large quantity of questions. It is difficult to ask revealing questions at the start of your analysis because you do not know what insights are contained in your dataset. On the other hand, each new question that you ask will expose you to a new aspect of your data and increase your chance of making a discovery.

However, two types of questions will always be useful for making discoveries within your data. You can loosely word these questions as:

	1. What type of variation occurs within my variables?

	2. What type of covariation occurs between my variables?

Variation

	Variation is the tendency of the values of a variable to change from measurement to measurement.

	Every variable has its own pattern of variation, which can reveal interesting information. The best way to understand that pattern is to visualise the distribution of the variable's values.

	Questions about histograms:

		- Which values are the most common? Why?

		- Which values are rare? Why? Does that match your expectations?

		- Can you see any unusual patterns? What might explain them?

	Clusters of similar values suggest that subgroups exist in your data. To understand the subgroups, ask:

		- How are the observations within each cluster similar to each other?

		- How are the observations in separate clusters different from each other?

		- How can you explain or describe the clusters?

		- Why might the appearance of clusters be misleading?

Outliers

	It's good practice to repeat your analysis with and without the outliers. If they have minimal effect on the results, and you can't figure out why they're there, it's reasonable to replace them with missing values, and move on. However, if they have a substantial effect on your results, you shouldn't drop them without justification. You'll need to figure out what caused them (e.g. a data entry error) and disclose that you removed them in your write-up.

Covariation

	If variation describes the behavior within a variable, covariation describes the behavior between variables. Covariation is the tendency for the values of two or more variables to vary together in a related way.

Visualisation and Plots

	histogram - for several histogram, use geom_freqpoly()

	density - is the count standardised so that the area under each frequency polygon

	is one.

	Two categoriacal variables

		geom_count()

		geom_tile() If the categorical variables are unordered, you might want to use the seriation package to simultaneously reorder the rows and columns in order to more clearly reveal interesting patterns. For larger plots, you might want to try the d3heatmap or heatmaply packages, which create interactive plots.

	Two continuous variables

		- geom_point()

		- For large datasets:	geom_bin2d() and geom_hex() divide the coordinate plane into 2d bins and then use a fill color to display how many points fall into each bin. geom_bin2d() creates rectangular bins. geom_hex() creates hexagonal bins. You will need to install the hexbin package to use geom_hex().

		- Another option is to bin one continuous variable so it acts like a categorical variable:

			- geom_boxplot(mapping = aes(group = cut_width(carat, 0.1)))

			- Another approach is to display approximately the same number of points in each bin: geom_boxplot(mapping = aes(group = cut_number(carat, 20)))

Patterns and models

	Patterns

		Patterns in your data provide clues about relationships. If a systematic relationship exists between two variables it will appear as a pattern in the data. If you spot a pattern, ask yourself:

			Could this pattern be due to coincidence (i.e. random chance)?

			How can you describe the relationship implied by the pattern?

			How strong is the relationship implied by the pattern?

			What other variables might affect the relationship?

			Does the relationship change if you look at individual subgroups of the data?

		Patterns provide one of the most useful tools for data scientists because they reveal covariation. If you think of variation as a phenomenon that creates uncertainty, covariation is a phenomenon that reduces it. If two variables covary, you can use the values of one variable to make better predictions about the values of the second. If the covariation is due to a causal relationship (a special case), then you can use the value of one variable to control the value of the second.

	Models

		Models are a tool for extracting patterns out of data.

		It's hard to understand the relationship between two variables when they are tightly related.

		It's possible to use a model to remove the very strong relationship between them so we can explore the subtleties that remain.

			This can be done by fitting a model that predicts one variable from another and then compute the residuals (the difference between the predicted value and the actual value). The residuals give us a view of the price of the diamond, once the effect of carat has been removed.

			library(modelr)

			mod \<- lm(log(price) \~ log(carat), data = diamonds)

			diamonds2 \<- diamonds %\>%

			 add_residuals(mod) %\>%

			 mutate(resid = exp(resid))

			ggplot(data = diamonds2) +

			 geom_point(mapping = aes(x = carat, y = resid))

			Once you've removed the strong relationship between carat and price, you can see what you expect in the relationship between cut and price: relative to their size, better quality diamonds are more expensive.

			ggplot(data = diamonds2) + geom_boxplot(mapping = aes(x = cut, y = resid))

Exploratory Data Analysis

	Diagram from: R for Data Science

								Explore & Understand

					 ,-\> Transform -\> Visualize -\> Model ----,

	Import -\> Tidy -\> \|		 								 \|	--\> COMMUNICATE

					 \|										 \|

					 '--- Model \<- Visualize \<- Transform \<-'

INTERACTIVE GRAPHICS AUGMENT EXPLORATION

	Interactive graphics \*can\* augment eploratory analysis, but are only \*pratical\* when we can iterate quickly

	iDENTIFY STRUCTURE THAT OTHERWISE GOES MISSING

	sEARCH FOR INFORMATION QUICKLY WITHOU FULLY SPECIFIED QUESTIONS

		mULTIPLE LINKED VIEWs are the optimal framework for posing queries about data

	Diagnose and understand models

	See: visual (Majumder et al 2013) and post-selection (Berk et al 2013) inference frameworks

	

	<https://plotly-book.cpsievert.me> \| <https://plotly-r.com/introduction.html>

Quantile Quantile Plots (qq-plots)

	To corroborate that a theoretical distribution, for example the normal distribution, is in fact a good approximation

	 Quantiles are best understood by considering the special case of percentiles. The p-th percentile of a list of a distribution is defined as the number q that is bigger than p% of numbers (so the inverse of the cumulative distribution function)

HOW TO DISPLAY DATA BADLY

	 Karl W. Broman - <http://kbroman.org/pages/talks.html>

	General principles

	The aims of good data graphics is to display data accurately and clearly. According to Karl, some rules for displaying data badly are:

		Display as little information as possible.

	 Obscure what you do show (with chart junk).

	 Use pseudo-3D and color gratuitously.

	 Make a pie chart (preferably in color and 3D).

	 Use a poorly chosen scale.

	 Ignore significant figures.

Displaying data well

In general, you should follow these principles:

Be accurate and clear.

Let the data speak.

Show as much information as possible, taking care not to obscure the message.

Science not sales: avoid unnecessary frills (esp. gratuitous 3D).

In tables, every digit should be meaningful. Don't drop ending 0's.

Some further reading:

ER Tufte (1983) The visual display of quantitative information. Graphics Press.

ER Tufte (1990) Envisioning information. Graphics Press.

ER Tufte (1997) Visual explanations. Graphics Press.

WS Cleveland (1993) Visualizing data. Hobart Press.

WS Cleveland (1994) The elements of graphing data. CRC Press.

A Gelman, C Pasarica, R Dodhia (2002) Let's practice what we preach: Turning tables into graphs. The American Statistician 56:121-130

NB Robbins (2004) Creating more effective graphs. Wiley.

Nature Methods columns

CORRELATIONS

	For two given highly correlated techinical replicates, To examine how well the second vector reproduces the first, we need to study the differences. We therefore should plot that instead. In this plot, MA plot, we plot the difference (in the log scale) versus the average.

	These are referred to as Bland-Altman plots, or MA plots in the genomics literature, and we will talk more about them later. "MA" stands for "minus" and "average" because in this plot, the y-axis is the difference between two samples on the log scale (the log ratio is the difference of the logs), and the x-axis is the average of the samples on the log scale. In this plot, we see that the typical difference in the log (base 2) scale between two replicated measures is about 1. This means that when measurements should be the same, we will, on average, observe 2 fold difference. We can now compare this variability to the differences we want to detect and decide if this technology is precise enough for our purposes.

Misunderstanding Correlation (Advanced)

The use of correlation to summarize reproducibility has become widespread in, for example, genomics. Despite its English language definition, mathematically, correlation is not necessarily informative with regards to reproducibility. Here we briefly describe three major problems.

The most egregious related mistake is to compute correlations of data that is not approximated by bi-variate normal data. As described above, averages, standard deviations and correlations are popular summary statistics for two-dimensional data because, for the bivariate normal distribution, these five parameters fully describe the distribution. However, there are many examples of data that are not well approximated by bivariate normal data. Gene expression data, for example, tends to have a distribution with a very fat right tail.

The standard way to quantify reproducibility between two sets of replicated measurements, say x1,...,xn

and y1,...,yn

, is simply to compute the distance between them:

∑i=1nd2i−−−−−√ with di=xi−yi

This metric decreases as reproducibility improves and it is 0 when the reproducibility is perfect. Another advantage of this metric is that if we divide the sum by N, we can interpret the resulting quantity as the standard deviation of the d1,...,dN

if we assume the d average out to 0. If the d

can be considered residuals, then this quantity is equivalent to the root mean squared error (RMSE), a summary statistic that has been around for over a century. Furthermore, this quantity will have the same units as our measurements resulting in a more interpretable metric.

Another limitation of the correlation is that it does not detect cases that are not reproducible due to average changes. The distance metric does detect these differences. We can rewrite:

1n∑i=1n(xi−yi)2=1n∑i=1n[(xi−μx)−(yi−μy)+(μx−μy)]2

with μx

and μy

the average of each list. Then we have:

1n∑i=1n(xi−yi)2=1n∑i=1n(xi−μx)2+1n∑i=1n(yi−μy)2+(μx−μy)2+1n∑i=1n(xi−μx)(yi−μy)

For simplicity, if we assume that the variance of both lists is 1, then this reduces to:

1n∑i=1n(xi−yi)2=2+(μx−μy)2−2ρ

with ρ

the correlation. So we see the direct relationship between distance and correlation. However, an important difference is that the distance contains the term (μx−μy)2

and, therefore, it can detect cases that are not reproducible due to large average changes.

Yet another reason correlation is not an optimal metric for reproducibility is the lack of units. To see this, we use a formula that relates the correlation of a variable with that variable, plus what is interpreted here as deviation: x

and y=x+d. The larger the variance of d, the less x+d reproduces x. Here the distance metric would depend only on the variance of d and would summarize reproducibility. However, correlation depends on the variance of x as well. If d is independent of x

, then

cor(x,y)=11+var(d)/var(x)−−−−−−−−−−−−−−√

This suggests that correlations near 1 do not necessarily imply reproducibility. Specifically, irrespective of the variance of d

, we can make the correlation arbitrarily close to 1 by increasing the variance of x.

Robust summaries and log transformation

	The normal approximation is often useful when analyzing life sciences data. However, due to the complexity of the measurement devices, it is also common to mistakenly observe data points generated by an undesired process. For example, a defect on a scanner can produce a handful of very high intensities or a PCR bias can lead to a fragment appearing much more often than others. We therefore may have situations that are approximated by, for example, 99 data points from a standard normal distribution and one large number. 	

	n statistics we refer to these type of points as outliers. A small number of outliers can throw off an entire analysis.

	The median

		The median, defined as the point having half the data larger and half the data smaller, is a summary statistic that is robust to outliers.

	The median absolute deviation

		The median absolute deviation (MAD) is a robust summary for the standard deviation. It is defined by computing the differences between each point and the median, and then taking the median of their absolute values:

		1.4826×median{\|Xi−median(Xi)\|}

		The number 1.4826

		is a scaling factor such that the MAD is an unbiased estimate of the standard deviation. Notice how much closer we are to 1 with the MAD:

	Spearman correlation

		Earlier we saw that the correlation is also sensitive to outliers.

		The Spearman correlation follows the general idea of median and MAD, that of using quantiles. The idea is simple: we convert each dataset to ranks and then compute correlation:

		So if these statistics are robust to outliers, why would we ever use the non-robust version? In general, if we know there are outliers, then median and MAD are recommended over the mean and standard deviation counterparts. However, there are examples in which robust statistics are less powerful than the non-robust versions.

	We also note that there is a large statistical literature on Robust Statistics that go far beyond the median and the MAD: <http://genomicsclass.github.io/book/pages/robust_summaries.html#foot>

	Symmetry of log ratios

		Ratios are not symmetric.

Rank tests

	Wilcoxon Rank Sum Test

		We learned how the sample mean and SD are susceptible to outliers. The t-test is based on these measures and is susceptible as well. The Wilcoxon rank test (equivalent to the Mann-Whitney test) provides an alternative

		The basic idea is to 1) combine all the data, 2) turn the values into ranks, 3) separate them back into their groups, and 4) compute the sum or average rank and perform a test.

### Workflow: EDA for Transcriptomics

1) Select meaninful variables from pheno data

	- check for pvca assumptions for pheno vars

	- identify categorical variables with constant values across all samples or specific for each sample

	- identify numerical variables with unique/constant values across all samples

	- give a warning for numerical variables with unique values, in case they are some sort of sample identification

	- identify variables with NA values or with NA percentage above some given threshold (values ranging from 0 to 1)

2) Density plot

	Microarray

		Density plots give you an idea about the signal distribution across a chip.

		The default density plot function in the limma package (plotDensities) in Bioconductor plots the distribution of the red and green signals for each chip.

		Arrays that have very different distributions in these images should be checked carefully in the other quality plots. Generally these arrays will show up as problematic and if so, should be removed before analysis.

3) Sample clustering

	Clustering methods are commonly used to look for patterns in microarray data. However, they can also be used to consider data quality. Hierarchical clustering across slides allows a look at the overall trends of the data. The expectation is that slides containing the most simliar samples (e.g. those from a replicate group exposed to the same treatment) will be most similar to each other. By checking how the slides actually group, you can identify issues such as confounding, where data is grouping according to "uninteresting" factors such as the date the experiment was run or the person who isolated the RNA. (Of course, your experimental design and protocols should remove any chance of this sort of thing happening....)

	Careful experimental design and scheduling should result in problems such as this being minimized. For example, if you are able to hybridize all the arrays in an experiment in a single day, using single batches of all reagents, then this is usually worth doing. If not, then careful randomisation should be applied before carrying out the experiment.

	With these data, it is likely that two routes could be considered. If there were enough replicates, the single batch of slides done separately could possibly be disregarded. The other option is to use statistical methods to account for the month "factor" and work on the remaining residuals in considering differential effects.

	- Heatmap (1 - Cor)

	- Dendrograms

		- Complete linakge - Euclidean distance

			 - Maximum dissimilarity between points in two sets used to determine which two sets should be merged.

			 - Often gives comparable cluster sizes.

			 - Less sensitive to outliers.

			 - Works better with spherical distributions.

		

		- Single linkage

			 - Minimum dissimilarity between points in two sets used to determine which two sets should be merged.

			 - Can handle diverse shapes.

			 - Very sensitive to outliers or noise.

			 - Often results in unbalanced clusters.

			 - Extended, trailing clusters in which observations fused one at a time-chaining.

		- Average linkage

			 - Average dissimilarity between points in two sets used to determine which two sets should be merged.

			 - A compromise between single and complete linkage.

			 - Less sensitive to outliers.

			 - Works better with spherical distributions.

			 - Similar linkage: Ward's linkage. Join objects that minimize Euclidean distance / average Euclidean distance

4) MA plot

	- Microarray data

		MAplots show the relationship of signal ratios to signal intensities, where values are usually reported on the log2 scale.

		For a large chip where most data is not expressed at different levels across the treatments, after normalisation you expect of the data to fall approximately along a straight horizontal line along 0. Before normalisation, you may see that the line along which the majority of the data is clustered is curved, reflecting the dye bias of some genes. One aim of normalisation with two colour arrays is to "straighten out" this line.

	- RNA-Seq data

5) PVCA

	- estimate the variability of experimental effects including batch

	- The PVCA approach can be used as a screening tool to determine which sources of variability (biological, technical or other) are most prominent in a given microarray data set.

	- leverages the strengths of two very popular data analysis methods:

		1. principal component analysis (PCA) is used to efficiently reduce data dimension with maintaining the majority of the variability in the data

		2. variance components analysis (VCA) fits a mixed linear model using factors of interest as random effects to estimate and partition the total variability.

	- Using the eigenvalues associated with their corresponding eigenvectors as weights, associated variations of all factors are standardized and the magnitude of each source of variability (including each batch effect) is presented as a proportion of total variance.

	- Although PVCA is a generic approach for quantifying the corresponding proportion of variation of each effect, it can be a handy assessment for estimating batch effect before and after batch normalization.

6) RLE

	- [RLE Plots: Visualising Unwanted Variation in High Dimensional Data](https://arxiv.org/pdf/1704.03590.pdf)

	- are a powerful tool for visualising such variation in high dimensional data

	- are particularly useful for assessing whether a procedure aimed at removing unwanted variation, i.e. a normalisation procedure, has been successful.

7) Multidimensional Scaling (MDS)

	- Alternative dimensionality reduction approach

	- Represents distances in 2D or 3D space

	- Starts from distance matrix (PCA uses data points)

8) EDA with mixOmics

	8.1 PCA

		Principal Component Analysis (Jolliffe, 2005) is primarily used to explore one single type of 'omics data (e.g. transcriptomics, proteomics, metabolomics, etc) and identify the largest sources of variation. PCA is a mathematical procedure that uses orthogonal linear transformation of data from possibly correlated variables into uncorrelated principal components (PCs). The first principal component explains as much of the variability in the data as possible, and each following PC explains as much of the remaining variability as possible. Only the PCs which explain the most variance are retained. This is why choosing the number of dimensions or components (ncomp) is crucial (see the function tune.pca, below).

		- In mixOmics, PCA is numerically solved in two ways:

			1. With singular value decomposition (SVD) of the data matrix,which is the most computationally efficient way and is also adopted by most softwares and the R function prcomp in the stat package.

			2. With the Non-linear Iterative Partial Least Squares (NIPALS) in the case of missing values, which uses an iterative power method. See Methods: Missing values.

		- Input data should be centered (center = TRUE) and possibly (sometimes preferably) scaled so that all variables have a unit variance. This is especially advised in the case where the variance is not homogeneous across variables (scale = TRUE). By default, the variables are centered and scaled in the function, but the user is free to choose other options.

		Choosing the optimal parameters

			We can obtain as many dimensions (i.e. number of PCs) as the minimum between the number of samples and variables. However, the goal is to reduce the complexity of the data and therefore summarize the data in fewer underlying dimension.

			The number of principal Components to retain (also called the number of dimensions) is therefore crucial when performing PCA. The function tune.pca will plot the barplot of the proportion of explained variance for min(n, p)principal components, where n is the number of samples, and p the number of variables.

			tune.pca(X, ncomp = 10, center = TRUE, scale = FALSE)

	8.2 IPCA

		Independant Principal Component Analysis

		In some case studies, we have identified some limitations when using PCA:

			- PCA assumes that gene expression follows a multivariate normal distribution and recent studies have demonstrated that microarray gene expression measurements follow instead a super-Gaussian distribution.

			- PCA decomposes the data based on the maximization of its variance. In some cases, the biological question may not be related to the highest variance in the data.

		Instead, we propose to apply Independent Principal Component Analysis (IPCA) which combines the advantages of both PCA and Independent Component Analysis (ICA). It uses ICA as a denoising process of the loading vectors produced by PCA to better highlight the important biological entities and reveal insightful patterns in the data. A sparse version is also proposed (sIPCA). This approach was proposed in collaboration with Eric F. Yao (QFAB and University of Shanghai).

		The algorithm of IPCA is as follows:

			1. The original data matrix is centered (by default).

			2. PCA is used to reduce dimension and generate the loading vectors.

			3. ICA (FastICA) is implemented on the loading vectors to generate independent loading vectors.

			4. The centered data matrix is projected on the independent loading vectors to obtain the independent principal components.

		

		IPCA offers a better visualization of the data than ICA and with a smaller number of components than PCA.

		

		Choosing the optimal parameters

			The number of variables to select is still an open issue. In Yao et al (2012) we proposed to use the Davies Bouldin measure which is an index of crisp cluster validity. This index compares the within-cluster scatter with the between-cluster separation.

		IPCA is of class sPCA and PCA, and most of the PCA graphical methods can be applied. The default algorithm to estimate the unmixing matrix is set to mode = 'deflation'. By default, the data are centered, but not necessarily scaled.

		Kurtosis

			The kurtosis measure is used to order the loading vectors to order the Independent Principal Components. We have shown that the kurtosis value is a good post hoc indicator of the number of components to choose, as a sudden drop in the values corresponds to irrelevant dimensions.

			

			ipca.res\$kurtosis

	8.3 PLS-DA

		PLS Discriminant Analysis (PLS-DA)

			Partial Least Squares was not originally designed for classification and discrimination problems, but has often been used for that purpose (Nguyen and Rocke, 2002; Tan et al., 2004). The response matrix Y is qualitative and is internally recoded as a dummy block matrix that records the membership of each observation, i.e. each of the response categories are coded via an indicator variable. The PLS regression (now PLS-DA) is then run as if Y was a continuous matrix and works well in practice for large data sets where Linear Discriminant Analysis faces collinearity issues.

				- PLS-Discriminant Analysis (PLS-DA, Barker and Rayens, 2003) is a linear classification model that is able to predict the class of new samples.

				- sparse PLS-DA (sPLS-DA) enables the selection of the most predictive or discriminative features in the data that help classify the samples (Lê Cao et al., 2011).

			Similar to a PLS-regression mode, the tuning parameters include the number of dimensions or components ncomp. We can rely on the estimation of the classification error rate using cross-validation.

				plsda.res \<- plsda(X, Y, ncomp = 5) where ncomp is the number of components wanted

			We use the function perf to evaluate a PLS-DA model, using 5-fold cross-validation repeated 10 times (for an accurate estimation, consider 50-100 repeats when using fold-CV -- no repeat is necessary with loo-CV). In addition, the function is useful to decide on the optimal number of components to choose (either for PLSDA or sPLSDA, see the case study). The Balanced Error Rate (BER) and overall error rate is displayed for all prediction distances (see ?predict and details in suppl info in Rohart et al. (2017)):

				set.seed(2543) for reproducibility here, only when the \`cpus' argument is not used

				perf.plsda \<- perf(plsda.res, validation = "Mfold", folds = 5, progressBar = FALSE, auc = TRUE, nrepeat = 10)

				

				perf.plsda.srbct\$error.rate error rates

				plot(perf.plsda, col = color.mixo(1:3), sd = TRUE, legend.position = "horizontal")

			Here ncomp = 4 with max distance seems to achieve the best classification performance and can be retained for the final model, or for the sparse PLSDA model.

			The AUROC can also be plotted, beware that it only complements the PLSDA performance results (see Rohart et al. (2017) for more details).

			

	9. MDP

See:

	[PCA, MDS, k-means, Hierarchical clustering and heatmap for microarray data](https://rstudio-pubs-static.s3.amazonaws.com/93706_e3f683a8d77244a5b993b20ad6278f4b.html)

	[A Tutorial Review of Microarray Data Analysis](http://www.ub.edu/stat/docencia/bioinformatica/microarrays/ADM/slides/A_Tutorial_Review_of_Microarray_data_Analysis_17-06-08.pdf)

## Dimensionality Reduction

- PCA (linear)

- t-SNE (non-parametric/ nonlinear)

- Sammon mapping (nonlinear)

- Isomap (nonlinear)

- LLE (nonlinear)

- CCA (nonlinear)

- SNE (nonlinear)

- MVU (nonlinear)

- Laplacian Eigenmaps (nonlinear)

PCA is a linear algorithm. It will not be able to interpret complex polynomial relationship between features. On the other hand, t-SNE is based on probability distributions with random walk on neighborhood graphs to find the structure within the data.

A major problem with, linear dimensionality reduction algorithms is that they concentrate on placing dissimilar data points far apart in a lower dimension representation. But in order to represent high dimension data on low dimension, non-linear manifold, it is important that similar datapoints must be represented close together, which is not what linear dimensionality reduction algorithms do.

Local approaches seek to map nearby points on the manifold to nearby points in the low-dimensional representation. Global approaches on the other hand attempt to preserve geometry at all scales, i.e mapping nearby points to nearby points and far away points to far away points

It is important to know that most of the nonlinear techniques other than t-SNE are not capable of retaining both the local and global structure of the data at the same time.

tSNE

	[Algorithm](https://www.analyticsvidhya.com/blog/2017/01/t-sne-implementation-r-python/)

		10. Common Fallacies

Following are a few common fallacies to avoid while interpreting the results of t-SNE:

For the algorithm to execute properly, the perplexity should be smaller than the number of points. Also, the suggested perplexity is in the range of (5 to 50)

Sometimes, different runs with same hyper parameters may produce different results.

Cluster sizes in any t-SNE plot must not be evaluated for standard deviation, dispersion or any other similar measures. This is because t-SNE expands denser clusters and contracts sparser clusters to even out cluster sizes. This is one of the reasons for the crisp and clear plots it produces.

Distances between clusters may change because global geometry is closely related to optimal perplexity. And in a dataset with many clusters with different number of elements one perplexity cannot optimize distances for all clusters.

Patterns may be found in random noise as well, so multiple runs of the algorithm with different sets of hyperparameter must be checked before deciding if a pattern exists in the data.

Different cluster shapes may be observed at different perplexity levels.

Topology cannot be analyzed based on a single t-SNE plot, multiple plots must be observed before making any assessment.

## Data Science Skills
	Descriptive Analytics
		- Analyze historical data to answer "WHAT HAS HAPPENED TILL NOW?"

	Predictive Analytics
		- "WHAT WILL HAPPEN IN THE FUTURE?"
	-> Mathematics
	-> Statistics
	-> Data handling
		- knowledge of ETL (Extract Transform and Load) operations on data and experience working with popular ETL tools
		- confortable in handling data from different sources and in different formats
		- Excellent knowledge of SQL
		- work with structured, semi-structured and unstructured data

		Bonus:
			- knowledge of Big Data tools and technologies
			- Experience with NoSQL databases such as HBase, Cassandra and MongoDB

	-> Expert in Analysing and Visualizing the data
		- Experience working with popular data analysis and visualization packages in python and R such as numpy, scipy, pandas, matplotlib, ggplot and others
		- Experience with popular data analysis and visualization tools such as Tableu, Microsoft Power BI, SAP BI, SAS BI, Oracle BI, QliView or any other popular BI tool

	-> Goog communication and storytelling skills

	-> Predictive analysics:
		- Artificial intelligence
		- data mining
		- machine learning
		- statistical modeling

		- exposure to popular predictive analytics tools

## Cross-Normalization
Some methods for cross-study normalisation:
	- combining gene expression measures across independent studies (Wang et al. or Stevens and Doerge)
		Wang et al. DOI: 10.1093/bioinformatics/bth381
		Stevens and Doerge DOI: 10.1186/1471-2105-6-57
	- combining other measures such as rank-ordering (as in RankProd)
		DOI: https://doi.org/10.1093/bioinformatics/btl476
	- or p-values (Rhodes et al.)
		PMID: 12154050
	- the Bayesian approaches (e.g. Conlon et al.).
		DOI: 10.1186/1471-2105-7-247

Every method has caveats, issues and the more trivial the solution the more caveats.

There is a BioConductor package called MADAM which implements some meta-analysis methods including the RankProducts approaches. There are also cross-study approaches implemented in the web-based analysis tool ArrayMining.net including empirical Bayes approaches (as in ComBat), median rank score normalisation, normalised discretization, quantile discretization - references to all of these are on the website.
	MADAM - doi:  10.1186/1751-0473-5-3


## Clustering
### Clustering Evaluation
Cluster evaluation

Ref.:
	- https://nlp.stanford.edu/IR-book/html/htmledition/contents-1.html
	- https://link.springer.com/article/10.1007/s40595-016-0086-9
	- http://datamining.rutgers.edu/publication/internalmeasures.pdf
	- https://cran.r-project.org/web/packages/clusterCrit/vignettes/clusterCrit.pdf

Goal: high intra-cluster similarity and low inter-cluster similarity

Criterions:
	- internal
		Internal validation is the other type clustering evaluation, where the evaluation of the clustering is compared only with the result itself, i.e., the structure of found clusters and their relations to each other. This is much more realistic and efficient in many real-world scenarios as it does not refer to any assumed references from outside which is not always feasible to obtain. Particularly, with the huge increase of the data size and dimensionality as in recent applications with streaming data outputs, one can hardly claim that a complete knowledge of the ground truth is available or always valid.
		Internal clustering validation is based only on the intrinsic information of the data. Since we can only refer to the input dataset itself, internal validation needs assumptions about a “good” structure of found clusters which are normally given by reference result in external validation. Two main concepts, the compactness and the separation, are the most popular ones. Most other concepts are actually just combinations of variations of these two

		- Compactness
			The Compactness measures how closely data points are grouped in a cluster. Grouped points in the cluster are supposed to be related to each other, by sharing a common feature which reflects a meaningful pattern in practice. Compactness is normally based on distances between in-cluster points. The very popular way of calculating the compactness is through variance, i.e., average distance to the mean, to estimate how objects are bonded together with its mean as its center. A small variance indicates a high compactness 

		- Separation
			The Separation measures how different the found clusters are from each other. Users of clustering algorithms are not interested in similar or vague patterns when clusters are not well-separated (cf. Fig. 2). A distinct cluster that is far from the others corresponds to a unique pattern. Similar to the compactness, the distances between objects are widely used to measure separation, e.g., pairwise distances between cluster centers, or pairwise minimum distances between objects in different clusters. Separation is an inter-cluster criterion in the sense of relation between clusters.

	- external
		The external validation, which compares the clustering result to a reference result which is considered as the ground truth. If the result is somehow similar to the reference, we regard this final output as a “good” clustering. This validation is straightforward when the similarity between two clusterings has been well-defined, however, it has fundamental caveat that the reference result is not provided in most real applications. Therefore, external evaluation is largely used for synthetic data and mostly for tuning clustering algorithms.

		1. Purity
			Each cluster is assigned to the class which is most frequent in the cluster, and then the accuracy of this assignment is measured by counting the number of correctly assigned documents and dividing by N.
			Bad clusterings have purity values close to 0, a perfect clustering has a purity of 1.
			High purity is easy to achieve when the number of clusters is large - in particular, purity is 1 if each document gets its own cluster. Thus, we cannot use purity to trade off the qualit of the clustering against the number of clusters.
			A measure that allows us to make this tradeoff is NMI.

		2. Normalized mutual information (NMI)
			NMI is always a number between 0 and 1.

		3. Rand index
			We want to assign two documents to the same cluster if and only if they are similar. A true positive (TP) decision assigns two similar documents to the same cluster, a true negative (TN) decision assigns two dissimilar documents to different clusters. There are two types of errors we can commit. A (FP) decision assigns two dissimilar documents to the same cluster. A (FN) decision assigns two similar documents to different clusters. The Rand index measures the percentage of decisions that are correct. That is, it is simply accuracy.
			The Rand index gives equal weight to false positives and false negatives. Separating similar documents is sometimes worse than putting pairs of dissimilar documents in the same cluster.

		4. F measure
			 We can use the F measure measuresperf to penalize false negatives more strongly than false positives by selecting a value $\beta > 1$, thus giving more weight to recall. 






































