# Systems Biology

## Systems Thinking

### MODELS

	ref: <https://r4ds.had.co.nz/model-intro.html>

The goal of a model is to provide a simple low-dimensional summary of a dataset. Ideally, the model will capture true "signals" (i.e. patterns generated by the phenomenon of interest), and ignore "noise" (i.e. random variation that you're not interested in).

	Types of models:

		- Predictive models: supervised, generate predictions.

		- Data discovery models: Unsupervised, these models don't make predictions, but instead help you discover interesting relationships within your data.

\- model basics:

	- how models work mechanistically, focussing on the important family of linear models.

	- general tools for gaining insight into what a predictive model tells you about your data, focussing on simple simulated datasets.

model building:

	- how to use models to pull out known patterns in real data. Once you have recognised an important pattern it's useful to make it explicit in a model, because then you can more easily see the subtler signals that remain.

many models:

	- how to use many simple models to help understand complex datasets. This is a powerful technique, but to access it you'll need to combine modelling and programming tools.

Hypothesis generation vs. hypothesis confirmation

	

	Traditionally, the focus of modelling is on inference, or for confirming that an hypothesis is true. Doing this correctly is not complicated, but it is hard. There is a pair of ideas that you must understand in order to do inference correctly:

		1. Each observation can either be used for exploration or confirmation, not both.

		2. You can use an observation as many times as you like for exploration, but you can only use it once for confirmation. As soon as you use an observation twice, you've switched from confirmation to exploration.

	This is necessary because to confirm a hypothesis you must use data independent of the data that you used to generate the hypothesis. Otherwise you will be over optimistic. There is absolutely nothing wrong with exploration, but you should never sell an exploratory analysis as a confirmatory analysis because it is fundamentally misleading.

	If you are serious about doing an confirmatory analysis, one approach is to split your data into three pieces before you begin the analysis:

		1. 60% of your data goes into a training (or exploration) set. You're allowed to do anything you like with this data: visualise it and fit tons of models to it.

		2. 20% goes into a query set. You can use this data to compare models or visualisations by hand, but you're not allowed to use it as part of an automated process.

		3. 20% is held back for a test set. You can only use this data ONCE, to test your final model.

	

	This partitioning allows you to explore the training data, occasionally generating candidate hypotheses that you check with the query set. When you are confident you have the right model, you can check it once with the test data.

	

	Note that even when doing confirmatory modelling, you will still need to do EDA. If you don't do any EDA you will remain blind to the quality problems with your data

MODEL BASICS

	There are 2 parts to a model:

		1. Define a family of models that express a precise, but generic, pattern that you wnat to capture.

			Exemple:

				- straight line: y = a1\*x + a2

				- quadratic curve: y = a1\*x\*\*a2

		2. Generate a fitted model by finding the model from the family that is the closes to your data.

			- This takes the generic model family and makes it specific:

				- straight line: y = 3\*x + 7

				- quadratic curve: y = 9\*x\*\*2

			- IMPORTANT: a fitted model is just the closest model from a family of models.

				- That implies that you have the "best" model (according to some criteria)

				- it doesn't imply that you have a good model

				- it certainly doesn't imply that the model is "true"

			- The goal of a model is not to uncover truth, but to discover a simple approximation that is still useful.

			- George Box:

				- "All models are wrong, but some are useful."

				- "Now it would be very remarkable if any system existing in the real world could be exactly represented by any simple model. However, cunningly chosen parsimonious models often do provide remarkably useful approximations. For example, the law PV = RT relating pressure P, volume V and temperature T of an "ideal" gas via a constant R is not exactly true for any real gas, but it frequently provides a useful approximation and furthermore its structure is informative since it springs from a physical view of the behavior of gas molecules.

				For such a model there is no need to ask the question "Is the model true?". If "truth" is to be the "whole truth" the answer must be "No". The only question of interest is "Is the model illuminating and useful?"".

	Predictions

		- The predictions tells you the pattern that the model has captured, and the residuals tell you what the model has missed.

			

		Residuals

			- It's also useful to see what the model doesn't capture, the so-called residuals which are left after subtracting the predictions from the data. Residuals are powerful because they allow us to use models to remove striking patterns so we can study the subtler trends that remain.

			- Plot x vs residuals: If This looks like random noise, then suggests that our model has done a good job of capturing the patterns in the dataset.

		

		- Continuous variables

		- Categorical Variables

			- Generating a function from a formula is straight forward when the predictor is continuous, but things get a bit more complicated when the predictor is categorical.

			- Effectively, a model with a categorical x will predict the mean value for each category. (Why? Because the mean minimises the root-mean-squared distance.) That's easy to see if we overlay the predictions on top of the original data

			- You can't make predictions about levels that you didn't observe. Sometimes you'll do this by accident so it's good to recognise this error message

		- Interactions

			continuous and categorical

				- What happens when you combine a continuous and a categorical variable?

				There are two possible models you could fit to this data:

					mod1 \<- lm(y \~ x1 + x2, data = sim3)

					mod2 \<- lm(y \~ x1 \* x2, data = sim3)

				When you add variables with +, the model will estimate each effect independent of all the others.

				It's possible to fit the so-called interaction by using \*. For example:

					 	y \~ x1 \* x2

					 is translated to

					 	y = a_0 + a_1 \* x1 + a_2 \* x2 + a_12 \* x1 \* x2.

					

					 - Note that whenever you use \*, both the interaction and the individual components are included in the model.

					 - Note that the model that uses + has the same slope for each line, but different intercepts. The model that uses \* has a different slope and intercept for each line.

			Two Continuous

			Transformations

				You can also perform transformations inside the model formula. For example, log(y) \~ sqrt(x1) + x2 is transformed to log(y) = a_1 + a_2 \* sqrt(x1) + a_3 \* x2. If your transformation involves +, \*, \^, or -, you'll need to wrap it in I() so R doesn't treat it like part of the model specification. For example, y \~ x + I(x \^ 2) is translated to y = a_1 + a_2 \* x + a_3 \* x\^2. If you forget the I() and specify y \~ x \^ 2 + x, R will compute y \~ x \* x + x. x \* x means the interaction of x with itself, which is the same as x. R automatically drops redundant variables so x + x become x, meaning that y \~ x \^ 2 + x specifies the function y = a_1 + a_2 \* x. That's probably not what you intended!

				- Transformations are useful because you can use them to approximate non-linear functions. If you've taken a calculus class, you may have heard of Taylor's theorem which says you can approximate any smooth function with an infinite sum of polynomials. That means you can use a polynomial function to get arbitrarily close to a smooth function by fitting an equation like y = a_1 + a_2 \* x + a_3 \* x\^2 + a_4 \* x \^ 3. Typing that sequence by hand is tedious, so R provides a helper function: poly()

				- However there's one major problem with using poly(): outside the range of the data, polynomials rapidly shoot off to positive or negative infinity. One safer alternative is to use the natural spline, splines::ns().

	Other model families

		This chapter has focussed exclusively on the class of linear models, which assume a relationship of the form y = a_1 \* x1 + a_2 \* x2 + ... + a_n \* xn. Linear models additionally assume that the residuals have a normal distribution, which we haven't talked about. There are a large set of model classes that extend the linear model in various interesting ways. Some of them are:

		Generalised linear models, e.g. stats::glm(). Linear models assume that the response is continuous and the error has a normal distribution. Generalised linear models extend linear models to include non-continuous responses (e.g. binary data or counts). They work by defining a distance metric based on the statistical idea of likelihood.

		Generalised additive models, e.g. mgcv::gam(), extend generalised linear models to incorporate arbitrary smooth functions. That means you can write a formula like y \~ s(x) which becomes an equation like y = f(x) and let gam() estimate what that function is (subject to some smoothness constraints to make the problem tractable).

		Penalised linear models, e.g. glmnet::glmnet(), add a penalty term to the distance that penalises complex models (as defined by the distance between the parameter vector and the origin). This tends to make models that generalise better to new datasets from the same population.

		Robust linear models, e.g. MASS:rlm(), tweak the distance to downweight points that are very far away. This makes them less sensitive to the presence of outliers, at the cost of being not quite as good when there are no outliers.

		Trees, e.g. rpart::rpart(), attack the problem in a completely different way than linear models. They fit a piece-wise constant model, splitting the data into progressively smaller and smaller pieces. Trees aren't terribly effective by themselves, but they are very powerful when used in aggregate by models like random forests (e.g. randomForest::randomForest()) or gradient boosting machines (e.g. xgboost::xgboost.)

		These models all work similarly from a programming perspective. Once you've mastered linear models, you should find it easy to master the mechanics of these other model classes. Being a skilled modeller is a mixture of some good general principles and having a big toolbox of techniques. Now that you've learned some general tools and one useful class of models, you can go on and learn more classes from other sources.

MODEL BUILDING

	We will take advantage of the fact that you can think about a model partitioning your data into pattern and residuals. We'll find patterns with visualisation, then make them concrete and precise with a model. We'll then repeat the process, but replace the old response variable with the residuals from the model. The goal is to transition from implicit knowledge in the data and your head to explicit knowledge in a quantitative model. This makes it easier to apply to new domains, and easier for others to use.

	For very large and complex datasets this will be a lot of work. There are certainly alternative approaches - a more machine learning approach is simply to focus on the predictive ability of the model. These approaches tend to produce black boxes: the model does a really good job at generating predictions, but you don't know why. This is a totally reasonable approach, but it does make it hard to apply your real world knowledge to the model. That, in turn, makes it difficult to assess whether or not the model will continue to work in the long-term, as fundamentals change. For most real models, I'd expect you to use some combination of this approach and a more classic automated approach.

	It's a challenge to know when to stop. You need to figure out when your model is good enough, and when additional investment is unlikely to pay off. I particularly like this quote from reddit user Broseidon241:

	"A long time ago in art class, my teacher told me "An artist needs to know when a piece is done. You can't tweak something into perfection - wrap it up. If you don't like it, do it over again. Otherwise begin something new". Later in life, I heard "A poor seamstress makes many mistakes. A good seamstress works hard to correct those mistakes. A great seamstress isn't afraid to throw out the garment and start over.""

	-- Broseidon241, <https://www.reddit.com/r/datascience/comments/4irajq>

MORE:

	- Statistical Modeling: A Fresh Approach by Danny Kaplan, <http://www.mosaic-web.org/go/StatisticalModeling/.> This book provides a gentle introduction to modelling, where you build your intuition, mathematical tools, and R skills in parallel. The book replaces a traditional "introduction to statistics" course, providing a curriculum that is up-to-date and relevant to data science.

	- An Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani, <http://www-bcf.usc.edu/~gareth/ISL/> (available online for free). This book presents a family of modern modelling techniques collectively known as statistical learning. For an even deeper understanding of the math behind the models, read the classic Elements of Statistical Learning by Trevor Hastie, Robert Tibshirani, and Jerome Friedman, <https://web.stanford.edu/~hastie/Papers/ESLII.pdf> (also available online for free).

	- Applied Predictive Modeling by Max Kuhn and Kjell Johnson, <http://appliedpredictivemodeling.com.> This book is a companion to the caret package and provides practical tools for dealing with real-life predictive modelling challenges.

MANY MODELS

	three powerful ideas that help you to work with large numbers of models with ease:

		1. Using many simple models to better understand complex datasets.

		2. Using list-columns to store arbitrary data structures in a data frame. For example, this will allow you to have a column that contains linear models.

		3. Using the broom package, by David Robinson, to turn models into tidy data. This is a powerful technique for working with large numbers of models because once you have tidy data, you can apply all of the techniques that you've learned about earlier in the book.

	Watch:

		<https://www.youtube.com/watch?v=jbkSRLYSojo>

### General Systems Theory

Chapter 1

- Systems Everywhere

'In one way or another, we are forced to deal with complexities, with "wholes" or "systems", in all fields of knowledge. This implies a basic re-orientation in scientific thinking.'

- On the History of Systems Theory

- Trends in Systems Theory

Applications of the analytical procedure depends on two conditions. The first is that interactions between "parts" be non-existent or weak enough to be neglrected for certain research purposes. Only under this condition, can the parts be "worked out", actually, logically, and mathematically, and then be "put together". The second conditions is that the relations describing the behavior of parts be linear; only then is the condition of summativity give, i.e., an equation describing the behavior of the total is of the same form as the equations describing the behavior of the parts; partial processes can be superimposed to obtain the total process, etc.

These conditions are not fulfilled in the entities called systems, i.e., coonsisting of parts "in interaction". The prototype of their description is a set of simultaneous differential equations, which are nonlinear in the general case. A system or "organized complexity" may be circumscribed by the existence of "strong interactions" or interactions which are "nontrivial", i.e., nonlinear. The methodological problem of systems theory, therefore, is to provide for problems which, compared with the analytical-summative ones of classical science, are of a more general nature.

There are various approaches to deal with such problems. We intentionally use the somewhat loose expression "approaches" because they are logically inhomogeneous, represent different conceptual models, mathematical techniques, general points of view, etc.; they are, however, in accord in being "systems theories":

	- "Classical" system theory: applies classical mathematics, i.e., calculus. Its aim is to state principles whch apply to systems in general or defined subclasses (e.g., closed and open systems), to provide techiniques for their investigation and description, and to apply these to concrete cases. Owing to the generality of such description, it may be stated that certain formal properties will apply to any entity qua system (or open system, or hierarchical system, etc.), even when its particular nature, parts, relations, etc., are unknown or not investigated.

	- Computerization and simulation: Sets of simultaneous differential equations as a way to "model" or define a system are, if linear, tiresome to solve even in the case of a few variables; if nonlinear, they are unsolvable except in special cases.

	- Compartment theory: the systems consists of subunits with certain boundary conditions between which transport processes take place. Such bcompartment systems may have, e.g., "catenary" or "mammillary" structure (chain of compartments or a central compartment communicating with a number or peripheral ones). Understandably, mathematical difficulties become prohibitive in the case of three-or multicompartment systems. Laplace transforms and introduction of net and graph theory make analysis possible.

	- Set theory: The general formal properties of systems, closed and open systems, etc., can be axiomatized in terms of set theory.

	- Graph theory: Many systems problems concern structural or topologic properties of systems, rahter than quantitative relations.

	- Net theory: is connected with set, graph, compartment, etc. theories and is applied to such systems as nervous networks.

	- Cybernetics: is a theory of control systems based on communication (transfer of information) between system and environment and within the system, and control (feedback) of the system's function in regard to environment.

	- Information theory: is based on the concept of information, defined by an expression isomorphic to negative entropy of thermodynamics. Hence the expectation that information my be used as measure of organization.

	- Theory of automata: is the theory of abstract automata, with input, output, possibly trial-and-error and learning. A general model is the Turing machine.

	- Game theory: it is concerned with the behavior of supposedly "rational" players to obtain maximal gains and minimal losses by appropriate strategies against the other player (or nature). Hence it concerns essencially a "system" of antagonistic "forces" with specifications.

	- Decision theory:

	- Queuing theory: concerns optimization of arrangments under conditions of crowding.

--------------------------------------------------------------

Chapter 2 - The Meaning of General Systems Theory

- The Quest for a General System Theory

Surveying the evolution of modern science, we encounter a surprising phenomenon: independently of each other, similar problems and conceptions have evolved in widely different fields.

It was the aim of classical physics eventually to resolve natural phenomena into a play of elementary units governed by "blind" laws of nature. This was expressed in the ideal of the Laplacean spirit which, from the position and momentum of particles, can predict the state of the universe at any point in time.

We can ask for principles applying to systems in general, irrespective of whether they are of physical, biological or siciological nature. If we pose this question and conveniently define the concept of system, we find that models, principles, and laws exist which apply to generalized systems irrespective of their particular kind, elements and the "forces" involved.

A consequence of the existence of general system properties is the appearance of structural similarities or isomorphisms in different fields. There are correspondeces in the principles that govern the behavior of entities that are, intrinsically, widely different.

- Aims of General System Theory

- Closed and Open Systems: Limitations of Conventional Physics

The principle of equifinality.

Living systems, maintaining themselves in a steady state, can avoid the increase of entropy, and may even develop towards states of increased order and organization.

- Information and Entropy

In many cases, the flow of information corresponds to a flow of energy. However, examples can easily be given where the flow of information is opposite to the flow of energy, or where information is transmitted without a flow of energy or matter. So information, in general, cannot be expressed in terms of energy. There is, however, another way to measure information, namely, in terms of decisions.

Entropy, as we have already defined, is a measure of disorder; hence negative entropy or information is a measure of order or of organization since the latter, compared to distribution at random, is an improbable state.

A great variety of systems in technology and in living nature follow the feedback scheme, and it is well-known that a new discipline, called Cybernetics, was introduced by Norbert Wiener to deal with these phenomena. The theory tries to show that mechanisms of a feedback nature are the base of teleological or purposeful behavior in man-made machines as well as in living organisms, and in social systems.

It can be shown that the primary regulations in organic systems, i.e., those which are most fundamental and primitive in embryonic development as well as in evolution, are of the nature of dynamic interaction. They are based upon the fact that the living organism is an open system, maintaining itself in, or approaching a steady state. Superposed are those regulations which we may call secondary, and which are controlled by fixed arrangements, specially of the feedback type. This state of affairs is a consequence of a general principle of organization which may be called progressive mechanization. At first, systems-biological, neurological, psychological or social - are governed by dynamic interacino of their components; later on, fixed arrangements and conditions of constraint are established which render the system and its parts more efficient, but also gradually diminish and eventually abolish its equipotentiality. Thus, dynamics is the broader aspect, since we can always arrive from general system laws to machine like function by introducing suitable conditions of constraint, but the opposite is not possible.

- Causality and Teleology

- What is Organization?

Characteristics of organization, whether of a living organism or a society, are notions like those of wholeness, growth, differentiation, hierarchical order, dominance, control, competition, etc.

- General System Theory and the Unity of Science

The total of observable events, shows structural uniformities, manifesting themselves by isomorphic traces of order in the different levels or realms.

We cannot reduce the biologica, behavioral, and social levels to the lowest level, that of the constructs and laws of physics. We can, however, find constructs and possibly laws within the individual levels.

The unifying principle is that we find organization at all levels.

--------------------------------------------------------------

Chapter 3 - Some System Concepts in Elementary Mathematical Consideration

- The System Concept

In dealing with complexes of "elements", three different kinds of distinction may be made:

	1. according to their number;

	2. according to their species;

	3. according to the relations of elements;

Summative characteristics of an element are those which are the same within and outside the complex; they may therefore be obtained by means of summation of characteristics and behavior of elements as known in isolation.

Constitutive characteristics are those which are dependent on the specific relations within the complex; for understanding such characteristics we therefore must know not only the parts, but also the relations.

If we know the total of parts contained in a system and the relations between them, the behavior of the system may be derived from the behavior of the parts.

A system can be defined as a complex of interacting elements. Interaction means that elements, p, stand in relations, R, so that behavior of an element p in R is different from its behavior in another relation, R'. If the behaviours in R and R' are not different, there is no interaction, and the elements behave independently with respect to the relations R and R'.

If we are speaking of "systems", we mean "whole" or "unities". Then it seems paradoxical that, with respect to a whole, the concept of competition between its parts is introduced. In fact, however, these apparently contradictory statements both belong to the essentials of systems. Every whole is based upon the competition of its elements, and presupposes the "strugle between parts" (Roux).

## Systems Biology

Systems Biology Overview

\#\#\#\# doi: 10.4137/BBI.S12467 \#\#\#\#

In saying that we understand a biological process, we usually mean that we are able to predict future events and manipulate the process into a desired direction. Thus, biological inquiry could be viewed as an attempt to understand how a biological system transits from one state to another. In attempting to understand these transitions, a simple and frequently used approach is to compare two states of a system.

A central chalenge in posed by omics data is how to navigate through the haystack of measurements (eg, differential expression between two states) to identify the needles comprised of the critical causal factors.

Network analysis is a powerful and general approach to this problem, in which the biological system is modeled as a network whose nodes represent dynamical units (eg, genes, proteins, metabolites, etc) and edges stand for links between them.

Multiple groups have been successfully using such methods to gain a systems-level understanding of biological processes and to reveal mechanisms of different diseases:

	Amit I, Garber M, Chevrier N, et al. Unbiased reconstruction of a mammalian transcriptional network mediating pathogen responses. Science. 2009;326(5950):257--63.

	Sumazin P, Yang X, Chiu HS, et al. An extensive microRNA-mediated network of RNA-RNA interactions regulates established oncogenic pathways in glioblastoma. Cell. 2011;147(2):370--81.

	Yang D, Sun Y, Hu L, et al. Integrated analyses identify a master microRNA regulatory network for the mesenchymal subtype in serous ovarian cancer. Cancer Cell. 2013;23(2):186--99.

Several recent discoveries ranging from genes that drive progression of different cancers:

	Mine KL, Shulzhenko N, Yambartsev A, et al. Gene network reconstruction reveals cell cycle and antiviral genes as major drivers of cervical cancer. Nat Commun. 2013;4:1806.

	Chen JC, Alvarez MJ, Talos F, et al. Identification of causal genetic drivers of human disease through systems-level analysis of regulatory networks. Cell. 2014;159(2):402--14.

to microbes and microbial genes that cause a human illness:

	Morgun A, Dzutsev A, Dong X, et al. Uncovering effects of antibiotics on the host and microbiota using transkingdom gene networks. Gut gutjnl-2014-308820; 2015.

became possible because of the predictive power of network analysis. In particular, such insights would be very difficult to achieve if analysis is limited to finding differentially expressed genes and follow-up data mining of those genes.

Types of Network Analysis:

	- Covariation network analysis

	- semantic networks

	- molecular interacion networks

Types of omics measurements that are ameaneble to network analysis:

	- microarrays

	- next generation sequencing (for genotyping, transcriptome profiling, or microbiome analysis)

	- mass spectrometry-based proteomics

	- metabolomics

Databases:

	- Gene Expression Omnibus (GEO) and Array Express (for transcriptomics and epigenomics datasets);

	- PRIDE47 (for pro-teomics datasets)

	- the Human Metabolome Database (for metabolomics datasets)

	- lipid MAPS49 (for lipidomics datasets)

	- molecular interaction data from the BioGRID50 or BioCyc databases can be used as a prior for edge reconstruction.

Network analysis consists of two fundamental stages: network reconstruction and network interrogation.

Integrative Networks

	Integrative networks are becoming more popular under the premise that the resulting networks more comprehensively describe the underlying biology:

		Tran LM, Zhang B, Zhang Z, et al. Inferring causal genomic alterations in breast cancer using gene expression data. BMC Syst Biol. 2011;5:121.

		Chen Y, Zhu J, Lum PY, et al. Variations in DNA elucidate molecular networks that cause disease. Nature. 2008;452(7186):429--35.

	Each type of omics measurement tech- nology has a specific procedure for transforming the raw data (eg, DNA sequences, mass spectrum peaks, spot fluorescence intensity for microarrays) to a consensus abundance or fre- quency measure for each feature. These methods are reviewed here:

		 Olson NE. The microarray data analysis process: from raw data to biological significance. NeuroRx. 2006;3(3):373--83.

		 Anders S, McCarthy DJ, Chen Y, et al. Count-based differential expression analysis of RNA sequencing data using R and Bioconductor. Nat Protoc. 2013;8(9):1765--86.

		 Hamady M, Knight R. Microbial community profiling for human microbiome projects: tools, techniques, and challenges. Genome Res. 2009;19(7):1141--52.

		 Hernandez P, Müller M, Appel RD. Automated protein identification by tandem mass spectrometry: issues and strategies. Mass Spectrom Rev. 2006;25(2):235--54.

1\. Network Reconstruction

	the data-driven discovery or inference of the entities/nodes (transcripts, pro-teins, genes, metabolites, or microbes) and relationships or edges between these entities that together constitute the biological network.

	- Normalization (pre-processing)

		+ log-transformation in order to stabilize variances when measurements span orders of magnitude.

		+ normalization to correct for sample-to-sample variation in the overall distribution of abundance values (or more generally, to normalize specific quantities that depend on the distribution):

			- Microarray:

				median normalization

				quantile normalization

				LOWESS normalization (Berger JA, Hautaniemi S, Jarvinen AK, Edgren H, Mitra SK, Astola J. Optimized LOWESS normalization parameter selection for DNA microarray data. BMC Bioinformatics. 2004;5:194)

			- RNA-Seq:

				reads per kilobase per million mapped reads (RPKM) (Mortazavi A, Williams BA, McCue K, Schaeffer L, Wold B. Mapping and quantifying mammalian transcriptomes by RNA-Seq. Nat Methods. 2008;5(7):621--)

				trimmed mean of M-values (TMM) (Robinson MD, Oshlack A. A scaling normalization method for differential expression analysis of RNA-seq data. Genome Biol. 2010;11(3):R25)

		 Lim WK, Wang K, Lefebvre C, Califano A. Comparative analysis of microarray normalization procedures: effects on reverse engineering gene networks. Bioinformatics. 2007;23(13):i282--8.

		 Dillies MA, Rau A, Aubert J, et al; French StatOmique Consortium. A comprehensive evaluation of normalization methods for illumina high-throughput RNA sequencing data analysis. Brief Bioinform. 2013;14(6):671--83

	- Discovery of Differentially expressed genes (selecting nodes)

		identification of the relevant subset of variables/genes that will constitute the nodes in the network. these would be genes for which there is significant differential expression between the sample groups.

		Statistical Tests:

			- Welch's t-test

			- moderated t-test

			- permutation tests

			--\> For parametric tests, accurate estimation of intra-sample-group variance is a critical issue; two improved variance estimation techniques are:

				+ locally pooled error

					Jain N, Thatte J, Braciale T, Ley K, O'Connell M, Lee JK. Local-pooled-error test for identifying differentially expressed genes with a small number of replicated microarrays. Bioinformatics. 2003;19(15):1945--51.

				+ empirical Bayes methods

				 	Smyth GK, Michaud J, Scott HS. Use of within-array replicate spots for assessing differential expression in microarray

				 	experiments. Bioinformatics. 2005;21(9):2067--75.

			Pan W. A comparative review of statistical methods for discovering differentially expressed genes in replicated microarray experiments. Bioinformatics. 2002;18(4):546--54.

		--\> Because omics data analysis typically involves tens of thousands of statistical tests, the correction for multiple hypotheses is essential.

			Dudoit S, Shaffer JP, Boldrick JC. Multiple hypothesis testing in microarray experiments. Stat Sci. 2003;18(1):71--103.

	- Correlation analysis for network reconstruction (finding links between nodes)

		The central biological principles underlying correlation network analysis are:

			1) that DEGs reflect functional changes

			2) that DEGs do not work individually but interact (eg, at the protein or pathway level) to function-ally alter the biological system.

		The central mathematical/statistical principle that allows us to use correlation networks for analysis of biological systems is that the correlation between two variables, if statistically significant, is always a result of causation. Specifically, correlation results from regulatory relations between the two variables, or from a common causal regulator to the two variables, or both, as in the case of a feed-forward loop.

			Pearl J. Causality: Models, Reasoning and Inference. Vol 29. Cambridge University Press, Cambridge, England; 2000

		

		To reconstruct the network, the Pearson or Spearman correlation coefficient can be used to obtain an association (similarity) measure for each possible pair of DEGs, with a cutoff for statistical significance and for a minimum correlation level. Together with the nodes, the edges whose similarity measures exceed this cutoff constitute a network.

		

		correlations should be calculated within a group of samples that belong to one class/biological state (pooling samples from different states/classes to compute the correlation coefficient leads to significant bias).

	- Discriminating between direct and inderect links

		Covariation gene networks in general consist of connections that result from a combination of direct and inderect effects between genes. Moreover, even if a true dependence exists between a pair of genes/nodes, its strengh estimation can be biased by additional indirect relationships:

			Pearl J. Direct and indirect effects. Paper presented at: Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence; 2001. Seattle, WA, August 2--5, 2001.

		For this reason, correlation networks in general have many edges that reflect indirect relationships between pairs of genes, where no direct relationship exists. Finding direct relation- ships between genes is important when one attempts to iden- tify causal gene regulators of a given biological process.

		Mathematically, direct effects can be defined as the association between two genes, holding the remaining genes constant.:

			Pearl J. An introduction to causal inference. Int J Biostat. 2010;6(2):7

		methods to discriminate between direct and indirect links in covariation networks:

			- partial correlation coefficient:

				De La Fuente A, Bing N, Hoeschele I, Mendes P. Discovery of meaningful associations in genomic data using partial correlation coefficients. Bioinformatics. 2004;20(18):3565--74.

				Marbach D, Costello JC, Küffner R, et al; DREAM5 Consortium. Wisdom of crowds for robust gene network inference. Nat Methods. 2012;9(8):796--804.

			- local partial correlation:

				 Thomas LD, Fossaluza V, Yambartsev A. Building complex networks through classical and Bayesian statistics-A comparison. Paper presented at: XI Brazilian Meeting on Bayesian Statistics. Amparo -- SP -- Brazil: EBEB; 2012.

			- others:

				Margolin AA, Nemenman I, Basso K, et al. ARACNE: an algorithm for the reconstruction of gene regulatory networks in a mammalian cellular context. BMC Bioinformatics. 2006;7(suppl 1):S7.

				Jang IS, Margolin A, Califano A. hARACNe: improving the accuracy of regulatory model reverse engineering via higher-order data processing inequality tests. Interface focus. 2013;3(4):20130011.

				Barzel B, Barabási A-L. Network link prediction by global silencing of indirect correlations. Nat Biotechnol. 2013;31(8):720--5.

				Feizi S, Marbach D, Médard M, Kellis M. Network deconvolution as a general method to distinguish direct dependencies in networks. Nat Biotechnol. 2013;31(8):726--33.

		

	- Proportion of unexpected correlations (improvement of reconstruction and error evaluation)

		A fundamental problem of the standard correlation network approach is that practical limita-tions in the numbers of sample measurements can lead to an unacceptably high error rate.

			+ proportion of unexpected correlations (PUC):

				allows identifying and removing approximately half of false positive edges from a covariation network with no reduc-tion in statistical power

				Yambartsev A, Perlin M, Kovchegov Y, Shulzhenko N, Mine KL, Morgun A. Unexpected links reflect the noise in networks. arXiv. 2013;1310.8341.

				The method takes into account a relation between the direction of regulation of two DEGs and the sign of correlation between the two genes. Thus, two up- and two downregulated genes must correlate positively; and a pair of oppositely regulated genes (one up-regulated and one down-regulated) should have negative correlation. Any devia- tion from this rule represents unexpected/erroneous edges and is removed from the network (Fig. 3).

	- Meta-analysis (improvement of reconstruction and error evaluation)

		A first step for meta-analysis we apply two filters:

			1) the same sign of statistic (mean, covariance, or correlation) throughout all datasets (ie, if gene A is upregulated in case over control in data-set 1, it should have the same direction of regulation in all other datasets to pass the filter)

			2) P-value thresholds across all datasets

		These filters provide consistency and control for heterogeneity across datasets for a given gene (or gene pair in case of correlation). The next step is an actual sta- tistical evaluation. In this step, meta-analysis combines common statistical measures, such as P-values, and calculate a weighted average for such measures. As a weighted average, we frequently use the Fisher's P-value calculation.

		After calculating Fisher's P-values for all genes, the standard FDR procedure can be used to adjust for multiple hypothesis testing.

		Several other approaches have been proposed for meta-analysis of gene expression data:

			Rhodes DR, Yu J, Shanker K, et al. Large-scale meta-analysis of cancer microarray data identifies common transcriptional profiles of neoplastic transformation and progression. Proc Natl Acad Sci U S A. 2004;101(25):9309--14.

			Hwang D, Rust AG, Ramsey S, et al. A data integration methodology for systems biology. Proc Natl Acad Sci U S A. 2005;102(48):17296--301.

	- Differentially coexpressed gene pairs (evaluating network changes)

		The networks discussed above model static correlations between genes that change their expression when the biological system transits from one state to another.

		However, the sets of edges within a gene covariation network can themselves vary from state to state, for example, when two genes are highly correlated in a subset of conditions but not across all conditions. Such a gene pair is called a differentially coexpressed gene pair:

			Kostka D, Spang R. Finding disease specific alterations in the co-expression of genes. Bioinformatics. 2004;20(suppl 1):i194--9

	

		It has been shown that differentially coexpressed gene pairs frequently play critical roles in pathogenesis.

		In order to search for differentially coexpressed gene pairs:

			- differentially associated pairs (DAPs):

				Skinner J, Kotliarov Y, Varma S, et al. Construct and compare gene coexpression networks with DAPfinder and DAPview. BMC Bioinformatics. 2011;12:286.

			- Other methods:

				Watson M. CoXpress: differential co-expression in gene expression data. BMC Bioinformatics. 2006;7:509.

				

	- Integrating heterogeneous omics data types: inter-omics networks

		An inter-omics network is a bipartite network in which each edge connects two nodes of different omics types

		Approaches for omics data integration generally fall into one of two modalities:

			+ first (and most prevalent) is integrating different types of data generated for a given gene/gene product. In other words, a given node pertains to more than one network (eg, measurements of the copy numbers of gene A and transcript levels of gene A pertain to genomic and transcriptomic networks, respectively).

			

			+ The other type of integration makes an edge/link between two nodes from different omics networks. We call the result of such integration an inter-omics network.

		There are two different approaches to infer such interomics links/edges:

			+ The first one is based on bringing into reconstruction an experimental result supporting a link between nodes of different omics. For example, nodes from proteo mics and metabolomics networks can be connected on the basis of the experiment showing that a specific protein is an enzyme necessary for the production of a given metabolite.

			+ The second approach, which infers edges between different omics, establishes connection between two different (knowledge- wise unrelated) quantitative variables based on their statistical association (eg, correlation between gene expression and abundance of metabolites measured in the same samples).

		

		Thus, the entire reconstruction procedure consists of inference on networks of each omics type separately, and then integration of these two networks into the inter-omics network. This is a straightforward and easily implementable algorithm. Furthermore, there is a popular tool, integr- Omics, that is used for heterogeneous data integration using partial least squares regression:

			Le Cao KA, Gonzalez I, Dejean S. integrOmics: an R package to unravel rela- tionships between two omics datasets. Bioinformatics. 2009;25(21):2855--6.

		Genome-wide measurements of epi-genetic marks and transcriptome data can be combined to

		elucidate mechanisms of gene regulation:

			 Ramsey SA, Knijnenburg TA, Kennedy KA, et al. Genome-wide histone acetylation data improve prediction of mammalian transcription factor binding sites. Bioinformatics. 2010;26(17):2071--5.

			 Shilatifard A. Chromatin modifications by methylation and ubiquitination: implications in the regulation of gene expression. Annu Rev Biochem. 2006;75:243--69.

			 Ramsey, Stephen A, et al. Epigenome-guided analysis of the transcriptome of plaque macrophages during atherosclerosis regression reveals activation of the Wnt signaling pathway. PLoS genetics. 2014;10(2):e1004828.

		

		Integration of gene copy number data (chromosomal aberrations) and gene expression measurements has enabled the discovery of key drivers:

			 Mine KL, Shulzhenko N, Yambartsev A, et al. Gene network reconstruction reveals cell cycle and antiviral genes as major drivers of cervical cancer. Nat Commun. 2013;4:1806.

			Akavia UD, Litvin O, Kim J, et al. An integrated approach to uncover drivers of cancer. Cell. 2010;143(6):1005--7.

		

		integration of metage-nomics data from gut microbiota with intestinal gene expres-sion can reveal new mechanisms of crosstalk between microbes and their hosts:

			 Morgun A, Dzutsev A, Dong X, et al. Uncovering effects of antibiotics on the host and microbiota using transkingdom gene networks. Gut gutjnl-2014-308820; 2015

		

		

2\. Network interrogation

--\> Systematic network analysis to gain maximal insights from a biological network that has been reconstructed

	- Revealing potential mechanisms of a biological process or disease

		+ quais são as vias envolvidas no processo?

		+ quais são os key nodes de tais vias?

		+ quais são as interações entre as vias identificadas. incluindo os nodos nas redes responsaveis por sua interação?

	- Which functional pathways are involved?

		--\> A principal vantagem de se fazer uma module network analysis, ao invés de simplesmente aplicar métodos de clusterização nos genes da tabela de expressão, é que, enquanto módulos incluiriam genes up e down regulated qua correspondem à potenciais relações estimulatórias e inibitórias dentro de uma dada via funcional, abordagens tradicionais de clusterização agrupariam genes com perfil de expressão similares, separando genes up de downs da mesma via em vias diferentes.

		+ Quais são as subredes mais densas (módulos/clusters)?

	- Enrichment analysis with external data

		--\> Performed by using literature-curated, gene-centric biological knowledge bases that connect genes to functional categories (terms) such as the functional terms in the Gene Ontology.

		--\> A gene can be enriched for a particular biochemical pathway, a location in a genome, or a location in a cellular compartment...

	- Key regulators of pathways/modules

		There are 2 major complementary approaches for finding key master regulators in covariation networks:

			1. Using network topology properties

			2. incorporating additional data into networks that provides information about causes of regulation for some nodes in a network

		Topological Properties:

			- degree and centrality measures:

				+ betweenness centrality

				+ closeness centrality

				+ eigenvector centrality

			--\> Nodes with high betweenness centrality (the so-called bottlenecks) have been shown to be predictive of gene essentiality

		

	- Integrating additional information in order to find causes of regulation

		--\> By overlaying such information on a coexpression network, one can establish the directionality of some edges, which improves the precision of identification of key regulators.

		types of biological information:

			+ genetic variants(aberrations, mutations, gene polymorphisms, etc)

				Mine KL, Shulzhenko N, Yambartsev A, et al. Gene network reconstruction reveals cell cycle and antiviral genes as major drivers of cervical cancer. Nat Commun. 2013;4:1806.

				Tran LM, Zhang B, Zhang Z, et al. Inferring causal genomic alterations in breast cancer using gene expression data. BMC Syst Biol. 2011;5:121

				Akavia UD, Litvin O, Kim J, et al. An integrated approach to uncover drivers of cancer. Cell. 2010;143(6):1005--7

			+ epigenetic modifications

			+ transcription factors

				Ramsey SA, Knijnenburg TA, Kennedy KA, et al. Genome-wide histone acetylation data improve prediction of mammalian transcription factor binding sites. Bioinformatics. 2010;26(17):2071--5.

				Carro MS, Lim WK, Alvarez MJ, et al. The transcriptional network for mesenchymal transformation of brain tumours. Nature. 2010;463(7279):318--25.

			+ other types of gene expression regulation (e.g., miRNA)

				Sumazin P, Yang X, Chiu HS, et al. An extensive microRNA-mediated network of RNA-RNA interactions regulates established oncogenic pathways in glioblastoma. Cell. 2011;147(2):370--81

				Lewis BP, Shih I-H, Jones-Rhoades MW, Bartel DP, Burge CB. Prediction of mammalian microRNA targets. Cell. 2003;115(7):787--98.

				John B, Enright AJ, Aravin A, Tuschl T, Sander C, Marks DS. Human microRNA targets. PLoS Biol. 2004;2(11):e363.

				Yang D, Sun Y, Hu L, et al. Integrated analyses identify a master microRNA regulatory network for the mesenchymal subtype in serous ovarian cancer. Cancer Cell. 2013;23(2):186--99

			+ binding sites (or computationally predicted binding sites) of transcription factors

	- How the pathways interact

		Dado que as redes representam modelos de mudanças globais nos sistemas biológicos, eles usualmente contém varios grupos de genes exercendo funções biológicas específicas. A cooperação dessas funções/vias tem um papel importante na regulação de processos biológicos. Portanto, uma rede transcricional pode ser vista mais como uma grupo de vias/módulos que interagem entre si do que grupos de genes individuais interagindo entre si.

	- Revealing function of individual node in the network

		network biology offers a novel way to infer functions for genes whose functions have been less studied. It uses the idea of guilty-by-association, in which genes that are located closely in a network may share a function.

		There are 2 major approaches that implement guilt by associntion for prediction of node function:

			+ direct approach:

				- neighboor counting; graphic algorithm; probabilistic methods;

				- they all assign a function to a node based on the functions of the of its direct neighboors.

			+ modular approch:

				- guide the assignment of a function to a gene by the collective function of other genes that belong to a given module in which the investigated gene is located.

	- Network cross-species conservation

		Assessment of evidence for network function

		subgraphs of the novel network (and in some approaches, constituent protein sequences) are used as keys to search for structural and component-sequence similarity to subgraphs in another species by searching for parsimonious subgraph-to-subgraph mappings (called a local network alignment).

		Alternatively, gene coexpression networks from two species can be compared in their entirety, to obtain a global alignment. A successful alignment enables all avail- able functional annotations in the orthologous subgraph to bring to bear on the functional interpretation of the novel net- work's subgraph.

	- What is the number of nodes needed to be perturbed in order to achieve a transition from one state of biological system to another? (This measure of network controllability (number of needed nodes))

		 Liu YY, Slotine JJ, Barabasi AL. Controllability of complex networks. Nature. 2011;473(7346):167--73

	- some mathematical properties observed in biological networks such as small world, scale-free, assortative mixing,102 and several others103 warrant further investigation to comprehend what types of environmental pressures led to selection of these properties during evolution and how they contribute to fitness and resilience of biological systems.

		102.Piraveenan M, Prokopenko M, Zomaya A. Assortative mixing in directed biological networks. IEEE/ACM Trans Comput Biol Bioinform. 2012;9(1):66--78.

		103. Newman ME. The structure and function of complex networks. SIAM Rev. 2003;45(2):167--256

### Hematopoiesis and its disorders_a systems biology approach

## Systems Vaccinology

### Systems Vaccinology - Pulendran, Nakaya - 2010

Understanding the immunological mehcanisms of vaccination is of paramount importance in the rational design of future vaccines against pandemics such as HIV, malaria, and tuberculosis and against emerging infections.

The innate immune sensing system is trhough the pattern recognition receptors (PRRs):

	- Toll-like receptors (TLRs)

	- C type lectin-like receptors

	- nucleotide-binding oligomerization domain-like receptors

	- retinoic acid-inducible gene I (RIG-I)-like receptors

Emerging evidence suggests that the nature of the DC subtype, as well as the particular PRR triggered, plays a critical role in modulating the strengh, quality, and persistence of adaptive immune responses.

Systems Biology:

	- The grand challenge of systems biology is to understand the biological complexity that emerges from interactions between our genomes and the environment.

	- Describes the complex interactions between all the parts in a biological system, with a view to elucidating new biological rules capable of predicting the behavior to the biological system.

	- A key goal is to understand the nature of biological networks.

Biological networks:

	- access, integrate, and communicate information from the genome to the environment and back.

	- These networks represent, in a sense, the lowest functional units of life processes, such as development, disease, immunity, and ageing.

	- Understanding these life processes requires understanding the nature and behavior of these networks, both their robustness and plasticity, in the face of a dynamic environment.

## Systems Immunology

## Artificial Biology

### Principles of Genetic Circuit Design

Principles of Genetic Circuit Design

\#\#\#\# --------- Need to know --------- \#\#\#\#

	- impact of an amino acid substitution on protein thermostability (Protein folding and de novo protein design for biotechnological applications.)

	- distribution of flux through modified metabolic networks (Constraining the metabolic genotype-phenotype relationship using a phylogeny of in silico methods.)

	- oscillators;

	- PID (proportional integral derivative) controller

\#\#\#\# -------------------------------- \#\#\#\#

	The production of bio-based chemicals can be improved by:

		- timming gene expression at different stages of fermentation;

		- turning on an enzyme only under particular conditions (e.g, high cell density);

	Synthetic regulation is also important to discover natural products like pharmaceuticals, insecticides and entirely new classes of chemicals. Accessing these products may require synthetic regulation because many of the relevant gene clusters are 'silent'.

	-\> 'Silent' gene clusters: the conditions under which they are induced are unknown;

	living cells could be programmed to serve as:

		- therapeutic agents that correct genetic disease;

		- colonize niches in the human microbiome to perform a therapeutic function;

	'smart' plants that sense and adapt to environmental challenges

	bacteria that organize to weave functional materials with nanoscale features

	1. circuits require the precise balancing of their component regulators to generate the proper response(46,47)

	2. many circuits are difficult to screen in directed-evolution experiments for correct perform- ance. Digital logic has clear ON and OFF states that can form the basis for a screen (12,53--59).

	3. there are few tools to measure circuit performance. Typically, a fluorescent reporter is used to measure the output, but fluorescence detection requires artificially high expression, and fluorescent protein deg- radation rates can limit the ability to measure dynamics.

	4. synthetic circuits are very sensitive to environment, growth condi- tions and genetic context in ways that are poorly understood61.

	- Transcriptional vs Post-trasncriptional circuits

	Transcriptional circuits maintain a common signal carrier, which simplifies the connection of circuits to build more sophisticated operations(72).

	Transcriptional circuits function by changing the flow of RNA polymerase (RNAP) on DNA.

	Post-transcriptional circuits, including those based on protein and RNA interactions, are covered in other excellent reviews(73--75).

Regulator Classes

	- DNA-binding proteins

		DNA-binding proteins can recruit or block RNAP to increase or decrease the flux, respectively.

		Manyy families of proteins can bind to specific DNA sequences (operators).

		The simplest way to use these proteins as regulators is to design promoters with operators that block the binding or progression of RNAP.

		Such repressors have been built out of:

			- zinc-finger proteins;

			- transcription activator-like effectors;

			- TetR homologs;

			- phage repressors;

			- LacI homologs;

		Expanding protein libraries can be challenging because each repressor has to be orthogonal; i.e., only interact with their operators and not the others in the set.

		There are also several challenges in using DNA-binding proteins to build circuits. Individual transcription factors may appear nontoxic, but often a combination of multiple regulators can lead to acute toxicity.

		The circuits can also be very dependent on growth rate because differences in the dilution rate change how quickly regulators accumulate or degrade, which alters their steady-state concentration, ultimately affecting their response.

		the response functions are often suboptimal and difficult to control because they have high OFF states (meaning they generate significant transcriptional signals in the OFF state) and low dynamic ranges.

		

	- Recombinases

		RNAP flux can also be altered with invertases that change the orientation of promoters, terminators or gene sequences.

		Recombinases are proteins that can facilitate the inversion of DNA segments between binding sites.

		Site specific recombinases often mediate 'cut-and-paste' recombination, during which DNA is looped, cleaved and religated (118).

		Two types of recombinases have been used to build genetic circuits:

			1. tyrosine recombinases (such as Cre, Flp and FimBE);

				This type require host-specific factors. These recombinases can be reversible and flip the DNA in both directions, or irreversible and flip in only a single direction.

			2. serine integrases;

				Catalyze unidirectional reactions that rely on double-strand breaks to invert DNA. Serine integrases typically do not require host factors and often have cognate excisionases that can be expressed independently to return the DNA to its original orientation.

		These proteins are ideal for memory storage because they flip DNA permanently, and once the DNA is flipped, its new orientation is maintained without the continuous input of materials or energy.

		using recombinases can be challenging because their reactions are slow (requiring 2--6 h) and often generate mixed populations when targeting a multicopy plasmid (121).

		Reversible recombinases can also generate mixed populations; however, this limitation was overcome recently when a unidirectional serine integrase was used to flip DNA in one direction and an integrase-excisionase pair was used to return it to the original state (124).

	- CRISPRi

		The CRISPRi system uses the Cas9 protein to bind to the DNA and alter transcription.

	- Adapted RNA-IN/OUT

Selecting parts to tune the circuit response

Common failures modes from connecting circuits

Interactions between synthetic circuits and the host organism
